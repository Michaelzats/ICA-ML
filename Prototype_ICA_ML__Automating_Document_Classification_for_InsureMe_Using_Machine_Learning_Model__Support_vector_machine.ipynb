{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeWkFjYBAh3cvYWzGEHRPJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Michaelzats/ICA-ML/blob/main/Prototype_ICA_ML__Automating_Document_Classification_for_InsureMe_Using_Machine_Learning_Model__Support_vector_machine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Document Classification using Machine Learning and Deep Learning Techniques**\n",
        "\n",
        "In the realm of automated document classification, we embarked on an exploration to classify a diverse set of documents into predefined categories. The dataset under scrutiny, sourced from Kaggle, is titled \"[(10)Dataset Text Document Classification](https://www.kaggle.com/datasets/jensenbaxter/10dataset-text-document-classification)\". It comprises documents categorized under the following labels: 'business', 'entertainment', 'food', 'graphics', 'historical', 'medical', 'politics', 'space', 'sport', and 'technology'. Each category is represented by 100 text files, culminating in a comprehensive dataset.\n",
        "\n",
        "For the purpose of training and evaluation, the dataset was partitioned into a 70-30 split, with 70% allocated for training and the remaining 30% reserved for testing.\n",
        "\n",
        "Our investigative approach encompassed a range of machine learning models, namely Naive Bayes, Support Vector Machines (SVM), and Random Forest. Additionally, a Deep Neural Network *(due to this model, the chanks of codes might take a while to load)*, a paradigm of deep learning, was also employed. To enhance the performance and robustness of these models, various techniques were integrated into the pipeline:\n",
        "\n",
        "Feature Engineering: Utilization of N-grams to capture local word order information.\n",
        "Word Embeddings: Leveraging pre-trained embeddings to represent words in a dense vector space.\n",
        "Feature Selection: The Chi-Squared Test was employed to select significant features.\n",
        "Model Ensembling: Bagging was used to reduce variance by training multiple models.\n",
        "Regularization Techniques: Dropout was introduced to prevent overfitting in the deep learning model.\n",
        "The models were evaluated on multiple metrics, including Precision, Recall, F1-Score, and Accuracy. The F1-Score, which harmoniously balances Precision and Recall, was chosen as the primary metric to determine the best model.\n",
        "\n",
        "Upon thorough evaluation, Dropout Deep Neural Network showed as the superior model with an F1 Score 0.983524. Nonetheless, as the model load time is significant, the second best model SVM without enhancement techniques is chosen with an F1 Score of 0.980188 is chosen as the best. While other models with feature enhancements were closely competitive, the computational overhead and marginally inferior performance make the standardized SVM the recommended choice for this document classification task.\n",
        "\n",
        "The development and refinement of our approach were greatly aided by resources from StackOverflow, ChatGPT, and a series of YouTube tutorials:\n",
        "\n",
        "[Python Machine Learning #4 - Support Vector Machines\n",
        "](https://www.youtube.com/watch?v=99Eyw7Quacc)\n",
        "\n",
        "[Neural Network Python | How to make a Neural Network in Python | Python Tutorial | Edureka](https://www.youtube.com/watch?v=9UBqkUJVP4g)\n",
        "\n",
        "[Machine Learning Tutorial Python - 11 Random Forest\n",
        "](https://www.youtube.com/watch?v=ok2s1vV9XW0)\n",
        "\n",
        "[Naive Bayes Classifier in Python (from scratch!)\n",
        "](https://www.youtube.com/watch?v=3I8oX3OUL6I)\n",
        "\n",
        "**Literature used for deeper understanding of the models:**\n",
        "Alpaydin, E., 2020. Introduction to machine learning. MIT press.\n",
        "Baltrušaitis, T., Ahuja, C. and Morency, L.P., 2018. Multimodal machine learning: A survey and taxonomy. IEEE transactions on pattern analysis and machine intelligence, 41(2), pp.423-443.\n",
        "Chen, Y.W. and Lin, C.J., 2006. Combining SVMs with various feature selection strategies. Feature extraction: foundations and applications, pp.315-324.\n",
        "Goodfellow, I., Bengio, Y. and Courville, A., 2016. Deep learning. MIT press.\n",
        "Liaw, A. and Wiener, M., 2015. randomForest: Breiman and Cutler’s random forests for classification and regression. R package version, 4, p.14.\n",
        "McCallum, A. and Nigam, K., 1998, July. A comparison of event models for naive bayes text classification. In AAAI-98 workshop on learning for text categorization (Vol. 752, No. 1, pp. 41-48).\n",
        "Oliphant, T.E., 2007. Python for scientific computing. Computing in science & engineering, 9(3), pp.10-20.\n",
        "Provost, F. and Fawcett, T., 2013. Data Science for Business: What you need to know about data mining and data-analytic thinking. \" O'Reilly Media, Inc.\".\n",
        "Shawe-Taylor, J. and Cristianini, N., 2004. Kernel methods for pattern analysis. Cambridge university press.\n",
        "Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. and Salakhutdinov, R., 2014. Dropout: a simple way to prevent neural networks from overfitting.\n",
        "The journal of machine learning research, 15(1), pp.1929-1958.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3fVk-W--FjsY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Libraries and API\n"
      ],
      "metadata": {
        "id": "FVW1yISQEj4B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mlXUj1a6EbNU"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries and API\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import LinearSVC\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Once uploaded, unzip using:\n",
        "#!unzip DATASET.zip\n",
        "#!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rD0A5RJaFmqo",
        "outputId": "2568dec3-31c9-4ae7-87b5-4d31b2104339"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  DATASET.zip\n",
            "  inflating: business/business_1.txt  \n",
            "  inflating: business/business_10.txt  \n",
            "  inflating: business/business_100.txt  \n",
            "  inflating: business/business_11.txt  \n",
            "  inflating: business/business_12.txt  \n",
            "  inflating: business/business_13.txt  \n",
            "  inflating: business/business_14.txt  \n",
            "  inflating: business/business_15.txt  \n",
            "  inflating: business/business_16.txt  \n",
            "  inflating: business/business_17.txt  \n",
            "  inflating: business/business_18.txt  \n",
            "  inflating: business/business_19.txt  \n",
            "  inflating: business/business_2.txt  \n",
            "  inflating: business/business_20.txt  \n",
            "  inflating: business/business_21.txt  \n",
            "  inflating: business/business_22.txt  \n",
            "  inflating: business/business_23.txt  \n",
            "  inflating: business/business_24.txt  \n",
            "  inflating: business/business_25.txt  \n",
            "  inflating: business/business_26.txt  \n",
            "  inflating: business/business_27.txt  \n",
            "  inflating: business/business_28.txt  \n",
            "  inflating: business/business_29.txt  \n",
            "  inflating: business/business_3.txt  \n",
            "  inflating: business/business_30.txt  \n",
            "  inflating: business/business_31.txt  \n",
            "  inflating: business/business_32.txt  \n",
            "  inflating: business/business_33.txt  \n",
            "  inflating: business/business_34.txt  \n",
            "  inflating: business/business_35.txt  \n",
            "  inflating: business/business_36.txt  \n",
            "  inflating: business/business_37.txt  \n",
            "  inflating: business/business_38.txt  \n",
            "  inflating: business/business_39.txt  \n",
            "  inflating: business/business_4.txt  \n",
            "  inflating: business/business_40.txt  \n",
            "  inflating: business/business_41.txt  \n",
            "  inflating: business/business_42.txt  \n",
            "  inflating: business/business_43.txt  \n",
            "  inflating: business/business_44.txt  \n",
            "  inflating: business/business_45.txt  \n",
            "  inflating: business/business_46.txt  \n",
            "  inflating: business/business_47.txt  \n",
            "  inflating: business/business_48.txt  \n",
            "  inflating: business/business_49.txt  \n",
            "  inflating: business/business_5.txt  \n",
            "  inflating: business/business_50.txt  \n",
            "  inflating: business/business_51.txt  \n",
            "  inflating: business/business_52.txt  \n",
            "  inflating: business/business_53.txt  \n",
            "  inflating: business/business_54.txt  \n",
            "  inflating: business/business_55.txt  \n",
            "  inflating: business/business_56.txt  \n",
            "  inflating: business/business_57.txt  \n",
            "  inflating: business/business_58.txt  \n",
            "  inflating: business/business_59.txt  \n",
            "  inflating: business/business_6.txt  \n",
            "  inflating: business/business_60.txt  \n",
            "  inflating: business/business_61.txt  \n",
            "  inflating: business/business_62.txt  \n",
            "  inflating: business/business_63.txt  \n",
            "  inflating: business/business_64.txt  \n",
            "  inflating: business/business_65.txt  \n",
            "  inflating: business/business_66.txt  \n",
            "  inflating: business/business_67.txt  \n",
            "  inflating: business/business_68.txt  \n",
            "  inflating: business/business_69.txt  \n",
            "  inflating: business/business_7.txt  \n",
            "  inflating: business/business_70.txt  \n",
            "  inflating: business/business_71.txt  \n",
            "  inflating: business/business_72.txt  \n",
            "  inflating: business/business_73.txt  \n",
            "  inflating: business/business_74.txt  \n",
            "  inflating: business/business_75.txt  \n",
            "  inflating: business/business_76.txt  \n",
            "  inflating: business/business_77.txt  \n",
            "  inflating: business/business_78.txt  \n",
            "  inflating: business/business_79.txt  \n",
            "  inflating: business/business_8.txt  \n",
            "  inflating: business/business_80.txt  \n",
            "  inflating: business/business_81.txt  \n",
            "  inflating: business/business_82.txt  \n",
            "  inflating: business/business_83.txt  \n",
            "  inflating: business/business_84.txt  \n",
            "  inflating: business/business_85.txt  \n",
            "  inflating: business/business_86.txt  \n",
            "  inflating: business/business_87.txt  \n",
            "  inflating: business/business_88.txt  \n",
            "  inflating: business/business_89.txt  \n",
            "  inflating: business/business_9.txt  \n",
            "  inflating: business/business_90.txt  \n",
            "  inflating: business/business_91.txt  \n",
            "  inflating: business/business_92.txt  \n",
            "  inflating: business/business_93.txt  \n",
            "  inflating: business/business_94.txt  \n",
            "  inflating: business/business_95.txt  \n",
            "  inflating: business/business_96.txt  \n",
            "  inflating: business/business_97.txt  \n",
            "  inflating: business/business_98.txt  \n",
            "  inflating: business/business_99.txt  \n",
            "  inflating: entertainment/entertainment_1.txt  \n",
            "  inflating: entertainment/entertainment_10.txt  \n",
            "  inflating: entertainment/entertainment_100.txt  \n",
            "  inflating: entertainment/entertainment_11.txt  \n",
            "  inflating: entertainment/entertainment_12.txt  \n",
            "  inflating: entertainment/entertainment_13.txt  \n",
            "  inflating: entertainment/entertainment_14.txt  \n",
            "  inflating: entertainment/entertainment_15.txt  \n",
            "  inflating: entertainment/entertainment_16.txt  \n",
            "  inflating: entertainment/entertainment_17.txt  \n",
            "  inflating: entertainment/entertainment_18.txt  \n",
            "  inflating: entertainment/entertainment_19.txt  \n",
            "  inflating: entertainment/entertainment_2.txt  \n",
            "  inflating: entertainment/entertainment_20.txt  \n",
            "  inflating: entertainment/entertainment_21.txt  \n",
            "  inflating: entertainment/entertainment_22.txt  \n",
            "  inflating: entertainment/entertainment_23.txt  \n",
            "  inflating: entertainment/entertainment_24.txt  \n",
            "  inflating: entertainment/entertainment_25.txt  \n",
            "  inflating: entertainment/entertainment_26.txt  \n",
            "  inflating: entertainment/entertainment_27.txt  \n",
            "  inflating: entertainment/entertainment_28.txt  \n",
            "  inflating: entertainment/entertainment_29.txt  \n",
            "  inflating: entertainment/entertainment_3.txt  \n",
            "  inflating: entertainment/entertainment_30.txt  \n",
            "  inflating: entertainment/entertainment_31.txt  \n",
            "  inflating: entertainment/entertainment_32.txt  \n",
            "  inflating: entertainment/entertainment_33.txt  \n",
            "  inflating: entertainment/entertainment_34.txt  \n",
            "  inflating: entertainment/entertainment_35.txt  \n",
            "  inflating: entertainment/entertainment_36.txt  \n",
            "  inflating: entertainment/entertainment_37.txt  \n",
            "  inflating: entertainment/entertainment_38.txt  \n",
            "  inflating: entertainment/entertainment_39.txt  \n",
            "  inflating: entertainment/entertainment_4.txt  \n",
            "  inflating: entertainment/entertainment_40.txt  \n",
            "  inflating: entertainment/entertainment_41.txt  \n",
            "  inflating: entertainment/entertainment_42.txt  \n",
            "  inflating: entertainment/entertainment_43.txt  \n",
            "  inflating: entertainment/entertainment_44.txt  \n",
            "  inflating: entertainment/entertainment_45.txt  \n",
            "  inflating: entertainment/entertainment_46.txt  \n",
            "  inflating: entertainment/entertainment_47.txt  \n",
            "  inflating: entertainment/entertainment_48.txt  \n",
            "  inflating: entertainment/entertainment_49.txt  \n",
            "  inflating: entertainment/entertainment_5.txt  \n",
            "  inflating: entertainment/entertainment_50.txt  \n",
            "  inflating: entertainment/entertainment_51.txt  \n",
            "  inflating: entertainment/entertainment_52.txt  \n",
            "  inflating: entertainment/entertainment_53.txt  \n",
            "  inflating: entertainment/entertainment_54.txt  \n",
            "  inflating: entertainment/entertainment_55.txt  \n",
            "  inflating: entertainment/entertainment_56.txt  \n",
            "  inflating: entertainment/entertainment_57.txt  \n",
            "  inflating: entertainment/entertainment_58.txt  \n",
            "  inflating: entertainment/entertainment_59.txt  \n",
            "  inflating: entertainment/entertainment_6.txt  \n",
            "  inflating: entertainment/entertainment_60.txt  \n",
            "  inflating: entertainment/entertainment_61.txt  \n",
            "  inflating: entertainment/entertainment_62.txt  \n",
            "  inflating: entertainment/entertainment_63.txt  \n",
            "  inflating: entertainment/entertainment_64.txt  \n",
            "  inflating: entertainment/entertainment_65.txt  \n",
            "  inflating: entertainment/entertainment_66.txt  \n",
            "  inflating: entertainment/entertainment_67.txt  \n",
            "  inflating: entertainment/entertainment_68.txt  \n",
            "  inflating: entertainment/entertainment_69.txt  \n",
            "  inflating: entertainment/entertainment_7.txt  \n",
            "  inflating: entertainment/entertainment_70.txt  \n",
            "  inflating: entertainment/entertainment_71.txt  \n",
            "  inflating: entertainment/entertainment_72.txt  \n",
            "  inflating: entertainment/entertainment_73.txt  \n",
            "  inflating: entertainment/entertainment_74.txt  \n",
            "  inflating: entertainment/entertainment_75.txt  \n",
            "  inflating: entertainment/entertainment_76.txt  \n",
            "  inflating: entertainment/entertainment_77.txt  \n",
            "  inflating: entertainment/entertainment_78.txt  \n",
            "  inflating: entertainment/entertainment_79.txt  \n",
            "  inflating: entertainment/entertainment_8.txt  \n",
            "  inflating: entertainment/entertainment_80.txt  \n",
            "  inflating: entertainment/entertainment_81.txt  \n",
            "  inflating: entertainment/entertainment_82.txt  \n",
            "  inflating: entertainment/entertainment_83.txt  \n",
            "  inflating: entertainment/entertainment_84.txt  \n",
            "  inflating: entertainment/entertainment_85.txt  \n",
            "  inflating: entertainment/entertainment_86.txt  \n",
            "  inflating: entertainment/entertainment_87.txt  \n",
            "  inflating: entertainment/entertainment_88.txt  \n",
            "  inflating: entertainment/entertainment_89.txt  \n",
            "  inflating: entertainment/entertainment_9.txt  \n",
            "  inflating: entertainment/entertainment_90.txt  \n",
            "  inflating: entertainment/entertainment_91.txt  \n",
            "  inflating: entertainment/entertainment_92.txt  \n",
            "  inflating: entertainment/entertainment_93.txt  \n",
            "  inflating: entertainment/entertainment_94.txt  \n",
            "  inflating: entertainment/entertainment_95.txt  \n",
            "  inflating: entertainment/entertainment_96.txt  \n",
            "  inflating: entertainment/entertainment_97.txt  \n",
            "  inflating: entertainment/entertainment_98.txt  \n",
            "  inflating: entertainment/entertainment_99.txt  \n",
            "  inflating: food/food_1.txt         \n",
            "  inflating: food/food_10.txt        \n",
            "  inflating: food/food_100.txt       \n",
            "  inflating: food/food_11.txt        \n",
            "  inflating: food/food_12.txt        \n",
            "  inflating: food/food_13.txt        \n",
            "  inflating: food/food_14.txt        \n",
            "  inflating: food/food_15.txt        \n",
            "  inflating: food/food_16.txt        \n",
            "  inflating: food/food_17.txt        \n",
            "  inflating: food/food_18.txt        \n",
            "  inflating: food/food_19.txt        \n",
            "  inflating: food/food_2.txt         \n",
            "  inflating: food/food_20.txt        \n",
            "  inflating: food/food_21.txt        \n",
            "  inflating: food/food_22.txt        \n",
            "  inflating: food/food_23.txt        \n",
            "  inflating: food/food_24.txt        \n",
            "  inflating: food/food_25.txt        \n",
            "  inflating: food/food_26.txt        \n",
            "  inflating: food/food_27.txt        \n",
            "  inflating: food/food_28.txt        \n",
            "  inflating: food/food_29.txt        \n",
            "  inflating: food/food_3.txt         \n",
            "  inflating: food/food_30.txt        \n",
            "  inflating: food/food_31.txt        \n",
            "  inflating: food/food_32.txt        \n",
            "  inflating: food/food_33.txt        \n",
            "  inflating: food/food_34.txt        \n",
            "  inflating: food/food_35.txt        \n",
            "  inflating: food/food_36.txt        \n",
            "  inflating: food/food_37.txt        \n",
            "  inflating: food/food_38.txt        \n",
            "  inflating: food/food_39.txt        \n",
            "  inflating: food/food_4.txt         \n",
            "  inflating: food/food_40.txt        \n",
            "  inflating: food/food_41.txt        \n",
            "  inflating: food/food_42.txt        \n",
            "  inflating: food/food_43.txt        \n",
            "  inflating: food/food_44.txt        \n",
            "  inflating: food/food_45.txt        \n",
            "  inflating: food/food_46.txt        \n",
            "  inflating: food/food_47.txt        \n",
            "  inflating: food/food_48.txt        \n",
            "  inflating: food/food_49.txt        \n",
            "  inflating: food/food_5.txt         \n",
            "  inflating: food/food_50.txt        \n",
            "  inflating: food/food_51.txt        \n",
            "  inflating: food/food_52.txt        \n",
            "  inflating: food/food_53.txt        \n",
            "  inflating: food/food_54.txt        \n",
            "  inflating: food/food_55.txt        \n",
            "  inflating: food/food_56.txt        \n",
            "  inflating: food/food_57.txt        \n",
            "  inflating: food/food_58.txt        \n",
            "  inflating: food/food_59.txt        \n",
            "  inflating: food/food_6.txt         \n",
            "  inflating: food/food_60.txt        \n",
            "  inflating: food/food_61.txt        \n",
            "  inflating: food/food_62.txt        \n",
            "  inflating: food/food_63.txt        \n",
            "  inflating: food/food_64.txt        \n",
            "  inflating: food/food_65.txt        \n",
            "  inflating: food/food_66.txt        \n",
            "  inflating: food/food_67.txt        \n",
            "  inflating: food/food_68.txt        \n",
            "  inflating: food/food_69.txt        \n",
            "  inflating: food/food_7.txt         \n",
            "  inflating: food/food_70.txt        \n",
            "  inflating: food/food_71.txt        \n",
            "  inflating: food/food_72.txt        \n",
            "  inflating: food/food_73.txt        \n",
            "  inflating: food/food_74.txt        \n",
            "  inflating: food/food_75.txt        \n",
            "  inflating: food/food_76.txt        \n",
            "  inflating: food/food_77.txt        \n",
            "  inflating: food/food_78.txt        \n",
            "  inflating: food/food_79.txt        \n",
            "  inflating: food/food_8.txt         \n",
            "  inflating: food/food_80.txt        \n",
            "  inflating: food/food_81.txt        \n",
            "  inflating: food/food_82.txt        \n",
            "  inflating: food/food_83.txt        \n",
            "  inflating: food/food_84.txt        \n",
            "  inflating: food/food_85.txt        \n",
            "  inflating: food/food_86.txt        \n",
            "  inflating: food/food_87.txt        \n",
            "  inflating: food/food_88.txt        \n",
            "  inflating: food/food_89.txt        \n",
            "  inflating: food/food_9.txt         \n",
            "  inflating: food/food_90.txt        \n",
            "  inflating: food/food_91.txt        \n",
            "  inflating: food/food_92.txt        \n",
            "  inflating: food/food_93.txt        \n",
            "  inflating: food/food_94.txt        \n",
            "  inflating: food/food_95.txt        \n",
            "  inflating: food/food_96.txt        \n",
            "  inflating: food/food_97.txt        \n",
            "  inflating: food/food_98.txt        \n",
            "  inflating: food/food_99.txt        \n",
            "  inflating: graphics/graphics_1.txt  \n",
            "  inflating: graphics/graphics_10.txt  \n",
            "  inflating: graphics/graphics_100.txt  \n",
            "  inflating: graphics/graphics_11.txt  \n",
            "  inflating: graphics/graphics_12.txt  \n",
            "  inflating: graphics/graphics_13.txt  \n",
            "  inflating: graphics/graphics_14.txt  \n",
            "  inflating: graphics/graphics_15.txt  \n",
            "  inflating: graphics/graphics_16.txt  \n",
            "  inflating: graphics/graphics_17.txt  \n",
            "  inflating: graphics/graphics_18.txt  \n",
            "  inflating: graphics/graphics_19.txt  \n",
            "  inflating: graphics/graphics_2.txt  \n",
            "  inflating: graphics/graphics_20.txt  \n",
            "  inflating: graphics/graphics_21.txt  \n",
            "  inflating: graphics/graphics_22.txt  \n",
            "  inflating: graphics/graphics_23.txt  \n",
            "  inflating: graphics/graphics_24.txt  \n",
            "  inflating: graphics/graphics_25.txt  \n",
            "  inflating: graphics/graphics_26.txt  \n",
            "  inflating: graphics/graphics_27.txt  \n",
            "  inflating: graphics/graphics_28.txt  \n",
            "  inflating: graphics/graphics_29.txt  \n",
            "  inflating: graphics/graphics_3.txt  \n",
            "  inflating: graphics/graphics_30.txt  \n",
            "  inflating: graphics/graphics_31.txt  \n",
            "  inflating: graphics/graphics_32.txt  \n",
            "  inflating: graphics/graphics_33.txt  \n",
            "  inflating: graphics/graphics_34.txt  \n",
            "  inflating: graphics/graphics_35.txt  \n",
            "  inflating: graphics/graphics_36.txt  \n",
            "  inflating: graphics/graphics_37.txt  \n",
            "  inflating: graphics/graphics_38.txt  \n",
            "  inflating: graphics/graphics_39.txt  \n",
            "  inflating: graphics/graphics_4.txt  \n",
            "  inflating: graphics/graphics_40.txt  \n",
            "  inflating: graphics/graphics_41.txt  \n",
            "  inflating: graphics/graphics_42.txt  \n",
            "  inflating: graphics/graphics_43.txt  \n",
            "  inflating: graphics/graphics_44.txt  \n",
            "  inflating: graphics/graphics_45.txt  \n",
            "  inflating: graphics/graphics_46.txt  \n",
            "  inflating: graphics/graphics_47.txt  \n",
            "  inflating: graphics/graphics_48.txt  \n",
            "  inflating: graphics/graphics_49.txt  \n",
            "  inflating: graphics/graphics_5.txt  \n",
            "  inflating: graphics/graphics_50.txt  \n",
            "  inflating: graphics/graphics_51.txt  \n",
            "  inflating: graphics/graphics_52.txt  \n",
            "  inflating: graphics/graphics_53.txt  \n",
            "  inflating: graphics/graphics_54.txt  \n",
            "  inflating: graphics/graphics_55.txt  \n",
            "  inflating: graphics/graphics_56.txt  \n",
            "  inflating: graphics/graphics_57.txt  \n",
            "  inflating: graphics/graphics_58.txt  \n",
            "  inflating: graphics/graphics_59.txt  \n",
            "  inflating: graphics/graphics_6.txt  \n",
            "  inflating: graphics/graphics_60.txt  \n",
            "  inflating: graphics/graphics_61.txt  \n",
            "  inflating: graphics/graphics_62.txt  \n",
            "  inflating: graphics/graphics_63.txt  \n",
            "  inflating: graphics/graphics_64.txt  \n",
            "  inflating: graphics/graphics_65.txt  \n",
            "  inflating: graphics/graphics_66.txt  \n",
            "  inflating: graphics/graphics_67.txt  \n",
            "  inflating: graphics/graphics_68.txt  \n",
            "  inflating: graphics/graphics_69.txt  \n",
            "  inflating: graphics/graphics_7.txt  \n",
            "  inflating: graphics/graphics_70.txt  \n",
            "  inflating: graphics/graphics_71.txt  \n",
            "  inflating: graphics/graphics_72.txt  \n",
            "  inflating: graphics/graphics_73.txt  \n",
            "  inflating: graphics/graphics_74.txt  \n",
            "  inflating: graphics/graphics_75.txt  \n",
            "  inflating: graphics/graphics_76.txt  \n",
            "  inflating: graphics/graphics_77.txt  \n",
            "  inflating: graphics/graphics_78.txt  \n",
            "  inflating: graphics/graphics_79.txt  \n",
            "  inflating: graphics/graphics_8.txt  \n",
            "  inflating: graphics/graphics_80.txt  \n",
            "  inflating: graphics/graphics_81.txt  \n",
            "  inflating: graphics/graphics_82.txt  \n",
            "  inflating: graphics/graphics_83.txt  \n",
            "  inflating: graphics/graphics_84.txt  \n",
            "  inflating: graphics/graphics_85.txt  \n",
            "  inflating: graphics/graphics_86.txt  \n",
            "  inflating: graphics/graphics_87.txt  \n",
            "  inflating: graphics/graphics_88.txt  \n",
            "  inflating: graphics/graphics_89.txt  \n",
            "  inflating: graphics/graphics_9.txt  \n",
            "  inflating: graphics/graphics_90.txt  \n",
            "  inflating: graphics/graphics_91.txt  \n",
            "  inflating: graphics/graphics_92.txt  \n",
            "  inflating: graphics/graphics_93.txt  \n",
            "  inflating: graphics/graphics_94.txt  \n",
            "  inflating: graphics/graphics_95.txt  \n",
            "  inflating: graphics/graphics_96.txt  \n",
            "  inflating: graphics/graphics_97.txt  \n",
            "  inflating: graphics/graphics_98.txt  \n",
            "  inflating: graphics/graphics_99.txt  \n",
            "  inflating: historical/historical_1.txt  \n",
            "  inflating: historical/historical_10.txt  \n",
            "  inflating: historical/historical_100.txt  \n",
            "  inflating: historical/historical_11.txt  \n",
            "  inflating: historical/historical_12.txt  \n",
            "  inflating: historical/historical_13.txt  \n",
            "  inflating: historical/historical_14.txt  \n",
            "  inflating: historical/historical_15.txt  \n",
            "  inflating: historical/historical_16.txt  \n",
            "  inflating: historical/historical_17.txt  \n",
            "  inflating: historical/historical_18.txt  \n",
            "  inflating: historical/historical_19.txt  \n",
            "  inflating: historical/historical_2.txt  \n",
            "  inflating: historical/historical_20.txt  \n",
            "  inflating: historical/historical_21.txt  \n",
            "  inflating: historical/historical_22.txt  \n",
            "  inflating: historical/historical_23.txt  \n",
            "  inflating: historical/historical_24.txt  \n",
            "  inflating: historical/historical_25.txt  \n",
            "  inflating: historical/historical_26.txt  \n",
            "  inflating: historical/historical_27.txt  \n",
            "  inflating: historical/historical_28.txt  \n",
            "  inflating: historical/historical_29.txt  \n",
            "  inflating: historical/historical_3.txt  \n",
            "  inflating: historical/historical_30.txt  \n",
            "  inflating: historical/historical_31.txt  \n",
            "  inflating: historical/historical_32.txt  \n",
            "  inflating: historical/historical_33.txt  \n",
            "  inflating: historical/historical_34.txt  \n",
            "  inflating: historical/historical_35.txt  \n",
            "  inflating: historical/historical_36.txt  \n",
            "  inflating: historical/historical_37.txt  \n",
            "  inflating: historical/historical_38.txt  \n",
            "  inflating: historical/historical_39.txt  \n",
            "  inflating: historical/historical_4.txt  \n",
            "  inflating: historical/historical_40.txt  \n",
            "  inflating: historical/historical_41.txt  \n",
            "  inflating: historical/historical_42.txt  \n",
            "  inflating: historical/historical_43.txt  \n",
            "  inflating: historical/historical_44.txt  \n",
            "  inflating: historical/historical_45.txt  \n",
            "  inflating: historical/historical_46.txt  \n",
            "  inflating: historical/historical_47.txt  \n",
            "  inflating: historical/historical_48.txt  \n",
            "  inflating: historical/historical_49.txt  \n",
            "  inflating: historical/historical_5.txt  \n",
            "  inflating: historical/historical_50.txt  \n",
            "  inflating: historical/historical_51.txt  \n",
            "  inflating: historical/historical_52.txt  \n",
            "  inflating: historical/historical_53.txt  \n",
            "  inflating: historical/historical_54.txt  \n",
            "  inflating: historical/historical_55.txt  \n",
            "  inflating: historical/historical_56.txt  \n",
            "  inflating: historical/historical_57.txt  \n",
            "  inflating: historical/historical_58.txt  \n",
            "  inflating: historical/historical_59.txt  \n",
            "  inflating: historical/historical_6.txt  \n",
            "  inflating: historical/historical_60.txt  \n",
            "  inflating: historical/historical_61.txt  \n",
            "  inflating: historical/historical_62.txt  \n",
            "  inflating: historical/historical_63.txt  \n",
            "  inflating: historical/historical_64.txt  \n",
            "  inflating: historical/historical_65.txt  \n",
            "  inflating: historical/historical_66.txt  \n",
            "  inflating: historical/historical_67.txt  \n",
            "  inflating: historical/historical_68.txt  \n",
            "  inflating: historical/historical_69.txt  \n",
            "  inflating: historical/historical_7.txt  \n",
            "  inflating: historical/historical_70.txt  \n",
            "  inflating: historical/historical_71.txt  \n",
            "  inflating: historical/historical_72.txt  \n",
            "  inflating: historical/historical_73.txt  \n",
            "  inflating: historical/historical_74.txt  \n",
            "  inflating: historical/historical_75.txt  \n",
            "  inflating: historical/historical_76.txt  \n",
            "  inflating: historical/historical_77.txt  \n",
            "  inflating: historical/historical_78.txt  \n",
            "  inflating: historical/historical_79.txt  \n",
            "  inflating: historical/historical_8.txt  \n",
            "  inflating: historical/historical_80.txt  \n",
            "  inflating: historical/historical_81.txt  \n",
            "  inflating: historical/historical_82.txt  \n",
            "  inflating: historical/historical_83.txt  \n",
            "  inflating: historical/historical_84.txt  \n",
            "  inflating: historical/historical_85.txt  \n",
            "  inflating: historical/historical_86.txt  \n",
            "  inflating: historical/historical_87.txt  \n",
            "  inflating: historical/historical_88.txt  \n",
            "  inflating: historical/historical_89.txt  \n",
            "  inflating: historical/historical_9.txt  \n",
            "  inflating: historical/historical_90.txt  \n",
            "  inflating: historical/historical_91.txt  \n",
            "  inflating: historical/historical_92.txt  \n",
            "  inflating: historical/historical_93.txt  \n",
            "  inflating: historical/historical_94.txt  \n",
            "  inflating: historical/historical_95.txt  \n",
            "  inflating: historical/historical_96.txt  \n",
            "  inflating: historical/historical_97.txt  \n",
            "  inflating: historical/historical_98.txt  \n",
            "  inflating: historical/historical_99.txt  \n",
            "  inflating: medical/medical_1.txt   \n",
            "  inflating: medical/medical_102.txt  \n",
            "  inflating: medical/medical_103.txt  \n",
            "  inflating: medical/medical_121.txt  \n",
            "  inflating: medical/medical_13.txt  \n",
            "  inflating: medical/medical_134.txt  \n",
            "  inflating: medical/medical_137.txt  \n",
            "  inflating: medical/medical_145.txt  \n",
            "  inflating: medical/medical_15.txt  \n",
            "  inflating: medical/medical_152.txt  \n",
            "  inflating: medical/medical_157.txt  \n",
            "  inflating: medical/medical_158.txt  \n",
            "  inflating: medical/medical_168.txt  \n",
            "  inflating: medical/medical_176.txt  \n",
            "  inflating: medical/medical_186.txt  \n",
            "  inflating: medical/medical_188.txt  \n",
            "  inflating: medical/medical_244.txt  \n",
            "  inflating: medical/medical_246.txt  \n",
            "  inflating: medical/medical_248.txt  \n",
            "  inflating: medical/medical_250.txt  \n",
            "  inflating: medical/medical_278.txt  \n",
            "  inflating: medical/medical_285.txt  \n",
            "  inflating: medical/medical_286.txt  \n",
            "  inflating: medical/medical_289.txt  \n",
            "  inflating: medical/medical_292.txt  \n",
            "  inflating: medical/medical_294.txt  \n",
            "  inflating: medical/medical_295.txt  \n",
            "  inflating: medical/medical_300.txt  \n",
            "  inflating: medical/medical_308.txt  \n",
            "  inflating: medical/medical_314.txt  \n",
            "  inflating: medical/medical_318.txt  \n",
            "  inflating: medical/medical_319.txt  \n",
            "  inflating: medical/medical_322.txt  \n",
            "  inflating: medical/medical_327.txt  \n",
            "  inflating: medical/medical_329.txt  \n",
            "  inflating: medical/medical_334.txt  \n",
            "  inflating: medical/medical_335.txt  \n",
            "  inflating: medical/medical_346.txt  \n",
            "  inflating: medical/medical_349.txt  \n",
            "  inflating: medical/medical_360.txt  \n",
            "  inflating: medical/medical_364.txt  \n",
            "  inflating: medical/medical_376.txt  \n",
            "  inflating: medical/medical_382.txt  \n",
            "  inflating: medical/medical_384.txt  \n",
            "  inflating: medical/medical_385.txt  \n",
            "  inflating: medical/medical_40.txt  \n",
            "  inflating: medical/medical_401.txt  \n",
            "  inflating: medical/medical_406.txt  \n",
            "  inflating: medical/medical_41.txt  \n",
            "  inflating: medical/medical_413.txt  \n",
            "  inflating: medical/medical_424.txt  \n",
            "  inflating: medical/medical_425.txt  \n",
            "  inflating: medical/medical_437.txt  \n",
            "  inflating: medical/medical_438.txt  \n",
            "  inflating: medical/medical_448.txt  \n",
            "  inflating: medical/medical_449.txt  \n",
            "  inflating: medical/medical_458.txt  \n",
            "  inflating: medical/medical_466.txt  \n",
            "  inflating: medical/medical_468.txt  \n",
            "  inflating: medical/medical_485.txt  \n",
            "  inflating: medical/medical_487.txt  \n",
            "  inflating: medical/medical_488.txt  \n",
            "  inflating: medical/medical_499.txt  \n",
            "  inflating: medical/medical_500.txt  \n",
            "  inflating: medical/medical_529.txt  \n",
            "  inflating: medical/medical_531.txt  \n",
            "  inflating: medical/medical_534.txt  \n",
            "  inflating: medical/medical_542.txt  \n",
            "  inflating: medical/medical_557.txt  \n",
            "  inflating: medical/medical_559.txt  \n",
            "  inflating: medical/medical_560.txt  \n",
            "  inflating: medical/medical_564.txt  \n",
            "  inflating: medical/medical_565.txt  \n",
            "  inflating: medical/medical_572.txt  \n",
            "  inflating: medical/medical_579.txt  \n",
            "  inflating: medical/medical_58.txt  \n",
            "  inflating: medical/medical_584.txt  \n",
            "  inflating: medical/medical_586.txt  \n",
            "  inflating: medical/medical_590.txt  \n",
            "  inflating: medical/medical_597.txt  \n",
            "  inflating: medical/medical_60.txt  \n",
            "  inflating: medical/medical_608.txt  \n",
            "  inflating: medical/medical_610.txt  \n",
            "  inflating: medical/medical_619.txt  \n",
            "  inflating: medical/medical_624.txt  \n",
            "  inflating: medical/medical_625.txt  \n",
            "  inflating: medical/medical_631.txt  \n",
            "  inflating: medical/medical_642.txt  \n",
            "  inflating: medical/medical_644.txt  \n",
            "  inflating: medical/medical_645.txt  \n",
            "  inflating: medical/medical_646.txt  \n",
            "  inflating: medical/medical_660.txt  \n",
            "  inflating: medical/medical_67.txt  \n",
            "  inflating: medical/medical_672.txt  \n",
            "  inflating: medical/medical_677.txt  \n",
            "  inflating: medical/medical_679.txt  \n",
            "  inflating: medical/medical_691.txt  \n",
            "  inflating: medical/medical_692.txt  \n",
            "  inflating: medical/medical_694.txt  \n",
            "  inflating: medical/medical_695.txt  \n",
            "  inflating: politics/politics_1.txt  \n",
            "  inflating: politics/politics_10.txt  \n",
            "  inflating: politics/politics_101.txt  \n",
            "  inflating: politics/politics_111.txt  \n",
            "  inflating: politics/politics_112.txt  \n",
            "  inflating: politics/politics_114.txt  \n",
            "  inflating: politics/politics_117.txt  \n",
            "  inflating: politics/politics_119.txt  \n",
            "  inflating: politics/politics_127.txt  \n",
            "  inflating: politics/politics_128.txt  \n",
            "  inflating: politics/politics_129.txt  \n",
            "  inflating: politics/politics_130.txt  \n",
            "  inflating: politics/politics_131.txt  \n",
            "  inflating: politics/politics_134.txt  \n",
            "  inflating: politics/politics_136.txt  \n",
            "  inflating: politics/politics_137.txt  \n",
            "  inflating: politics/politics_138.txt  \n",
            "  inflating: politics/politics_139.txt  \n",
            "  inflating: politics/politics_142.txt  \n",
            "  inflating: politics/politics_144.txt  \n",
            "  inflating: politics/politics_149.txt  \n",
            "  inflating: politics/politics_15.txt  \n",
            "  inflating: politics/politics_151.txt  \n",
            "  inflating: politics/politics_153.txt  \n",
            "  inflating: politics/politics_154.txt  \n",
            "  inflating: politics/politics_155.txt  \n",
            "  inflating: politics/politics_160.txt  \n",
            "  inflating: politics/politics_168.txt  \n",
            "  inflating: politics/politics_17.txt  \n",
            "  inflating: politics/politics_172.txt  \n",
            "  inflating: politics/politics_173.txt  \n",
            "  inflating: politics/politics_175.txt  \n",
            "  inflating: politics/politics_180.txt  \n",
            "  inflating: politics/politics_182.txt  \n",
            "  inflating: politics/politics_188.txt  \n",
            "  inflating: politics/politics_189.txt  \n",
            "  inflating: politics/politics_19.txt  \n",
            "  inflating: politics/politics_192.txt  \n",
            "  inflating: politics/politics_193.txt  \n",
            "  inflating: politics/politics_2.txt  \n",
            "  inflating: politics/politics_20.txt  \n",
            "  inflating: politics/politics_201.txt  \n",
            "  inflating: politics/politics_204.txt  \n",
            "  inflating: politics/politics_205.txt  \n",
            "  inflating: politics/politics_208.txt  \n",
            "  inflating: politics/politics_209.txt  \n",
            "  inflating: politics/politics_21.txt  \n",
            "  inflating: politics/politics_210.txt  \n",
            "  inflating: politics/politics_211.txt  \n",
            "  inflating: politics/politics_212.txt  \n",
            "  inflating: politics/politics_217.txt  \n",
            "  inflating: politics/politics_220.txt  \n",
            "  inflating: politics/politics_221.txt  \n",
            "  inflating: politics/politics_222.txt  \n",
            "  inflating: politics/politics_225.txt  \n",
            "  inflating: politics/politics_226.txt  \n",
            "  inflating: politics/politics_229.txt  \n",
            "  inflating: politics/politics_230.txt  \n",
            "  inflating: politics/politics_231.txt  \n",
            "  inflating: politics/politics_232.txt  \n",
            "  inflating: politics/politics_235.txt  \n",
            "  inflating: politics/politics_236.txt  \n",
            "  inflating: politics/politics_237.txt  \n",
            "  inflating: politics/politics_239.txt  \n",
            "  inflating: politics/politics_242.txt  \n",
            "  inflating: politics/politics_247.txt  \n",
            "  inflating: politics/politics_249.txt  \n",
            "  inflating: politics/politics_250.txt  \n",
            "  inflating: politics/politics_262.txt  \n",
            "  inflating: politics/politics_265.txt  \n",
            "  inflating: politics/politics_266.txt  \n",
            "  inflating: politics/politics_269.txt  \n",
            "  inflating: politics/politics_271.txt  \n",
            "  inflating: politics/politics_272.txt  \n",
            "  inflating: politics/politics_273.txt  \n",
            "  inflating: politics/politics_275.txt  \n",
            "  inflating: politics/politics_277.txt  \n",
            "  inflating: politics/politics_282.txt  \n",
            "  inflating: politics/politics_283.txt  \n",
            "  inflating: politics/politics_287.txt  \n",
            "  inflating: politics/politics_288.txt  \n",
            "  inflating: politics/politics_293.txt  \n",
            "  inflating: politics/politics_294.txt  \n",
            "  inflating: politics/politics_298.txt  \n",
            "  inflating: politics/politics_301.txt  \n",
            "  inflating: politics/politics_303.txt  \n",
            "  inflating: politics/politics_307.txt  \n",
            "  inflating: politics/politics_31.txt  \n",
            "  inflating: politics/politics_313.txt  \n",
            "  inflating: politics/politics_316.txt  \n",
            "  inflating: politics/politics_317.txt  \n",
            "  inflating: politics/politics_319.txt  \n",
            "  inflating: politics/politics_321.txt  \n",
            "  inflating: politics/politics_322.txt  \n",
            "  inflating: politics/politics_325.txt  \n",
            "  inflating: politics/politics_326.txt  \n",
            "  inflating: politics/politics_329.txt  \n",
            "  inflating: politics/politics_335.txt  \n",
            "  inflating: politics/politics_34.txt  \n",
            "  inflating: politics/politics_357.txt  \n",
            "  inflating: space/space_1.txt       \n",
            "  inflating: space/space_10.txt      \n",
            "  inflating: space/space_100.txt     \n",
            "  inflating: space/space_11.txt      \n",
            "  inflating: space/space_12.txt      \n",
            "  inflating: space/space_13.txt      \n",
            "  inflating: space/space_14.txt      \n",
            "  inflating: space/space_15.txt      \n",
            "  inflating: space/space_16.txt      \n",
            "  inflating: space/space_17.txt      \n",
            "  inflating: space/space_18.txt      \n",
            "  inflating: space/space_19.txt      \n",
            "  inflating: space/space_2.txt       \n",
            "  inflating: space/space_20.txt      \n",
            "  inflating: space/space_21.txt      \n",
            "  inflating: space/space_22.txt      \n",
            "  inflating: space/space_23.txt      \n",
            "  inflating: space/space_24.txt      \n",
            "  inflating: space/space_25.txt      \n",
            "  inflating: space/space_26.txt      \n",
            "  inflating: space/space_27.txt      \n",
            "  inflating: space/space_28.txt      \n",
            "  inflating: space/space_29.txt      \n",
            "  inflating: space/space_3.txt       \n",
            "  inflating: space/space_30.txt      \n",
            "  inflating: space/space_31.txt      \n",
            "  inflating: space/space_32.txt      \n",
            "  inflating: space/space_33.txt      \n",
            "  inflating: space/space_34.txt      \n",
            "  inflating: space/space_35.txt      \n",
            "  inflating: space/space_36.txt      \n",
            "  inflating: space/space_37.txt      \n",
            "  inflating: space/space_38.txt      \n",
            "  inflating: space/space_39.txt      \n",
            "  inflating: space/space_4.txt       \n",
            "  inflating: space/space_40.txt      \n",
            "  inflating: space/space_41.txt      \n",
            "  inflating: space/space_42.txt      \n",
            "  inflating: space/space_43.txt      \n",
            "  inflating: space/space_44.txt      \n",
            "  inflating: space/space_45.txt      \n",
            "  inflating: space/space_46.txt      \n",
            "  inflating: space/space_47.txt      \n",
            "  inflating: space/space_48.txt      \n",
            "  inflating: space/space_49.txt      \n",
            "  inflating: space/space_5.txt       \n",
            "  inflating: space/space_50.txt      \n",
            "  inflating: space/space_51.txt      \n",
            "  inflating: space/space_52.txt      \n",
            "  inflating: space/space_53.txt      \n",
            "  inflating: space/space_54.txt      \n",
            "  inflating: space/space_55.txt      \n",
            "  inflating: space/space_56.txt      \n",
            "  inflating: space/space_57.txt      \n",
            "  inflating: space/space_58.txt      \n",
            "  inflating: space/space_59.txt      \n",
            "  inflating: space/space_6.txt       \n",
            "  inflating: space/space_60.txt      \n",
            "  inflating: space/space_61.txt      \n",
            "  inflating: space/space_62.txt      \n",
            "  inflating: space/space_63.txt      \n",
            "  inflating: space/space_64.txt      \n",
            "  inflating: space/space_65.txt      \n",
            "  inflating: space/space_66.txt      \n",
            "  inflating: space/space_67.txt      \n",
            "  inflating: space/space_68.txt      \n",
            "  inflating: space/space_69.txt      \n",
            "  inflating: space/space_7.txt       \n",
            "  inflating: space/space_70.txt      \n",
            "  inflating: space/space_71.txt      \n",
            "  inflating: space/space_72.txt      \n",
            "  inflating: space/space_73.txt      \n",
            "  inflating: space/space_74.txt      \n",
            "  inflating: space/space_75.txt      \n",
            "  inflating: space/space_76.txt      \n",
            "  inflating: space/space_77.txt      \n",
            "  inflating: space/space_78.txt      \n",
            "  inflating: space/space_79.txt      \n",
            "  inflating: space/space_8.txt       \n",
            "  inflating: space/space_80.txt      \n",
            "  inflating: space/space_81.txt      \n",
            "  inflating: space/space_82.txt      \n",
            "  inflating: space/space_83.txt      \n",
            "  inflating: space/space_84.txt      \n",
            "  inflating: space/space_85.txt      \n",
            "  inflating: space/space_86.txt      \n",
            "  inflating: space/space_87.txt      \n",
            "  inflating: space/space_88.txt      \n",
            "  inflating: space/space_89.txt      \n",
            "  inflating: space/space_9.txt       \n",
            "  inflating: space/space_90.txt      \n",
            "  inflating: space/space_91.txt      \n",
            "  inflating: space/space_92.txt      \n",
            "  inflating: space/space_93.txt      \n",
            "  inflating: space/space_94.txt      \n",
            "  inflating: space/space_95.txt      \n",
            "  inflating: space/space_96.txt      \n",
            "  inflating: space/space_97.txt      \n",
            "  inflating: space/space_98.txt      \n",
            "  inflating: space/space_99.txt      \n",
            "  inflating: sport/sport_1.txt       \n",
            "  inflating: sport/sport_10.txt      \n",
            "  inflating: sport/sport_100.txt     \n",
            "  inflating: sport/sport_11.txt      \n",
            "  inflating: sport/sport_12.txt      \n",
            "  inflating: sport/sport_13.txt      \n",
            "  inflating: sport/sport_14.txt      \n",
            "  inflating: sport/sport_15.txt      \n",
            "  inflating: sport/sport_16.txt      \n",
            "  inflating: sport/sport_17.txt      \n",
            "  inflating: sport/sport_18.txt      \n",
            "  inflating: sport/sport_19.txt      \n",
            "  inflating: sport/sport_2.txt       \n",
            "  inflating: sport/sport_20.txt      \n",
            "  inflating: sport/sport_21.txt      \n",
            "  inflating: sport/sport_22.txt      \n",
            "  inflating: sport/sport_23.txt      \n",
            "  inflating: sport/sport_24.txt      \n",
            "  inflating: sport/sport_25.txt      \n",
            "  inflating: sport/sport_26.txt      \n",
            "  inflating: sport/sport_27.txt      \n",
            "  inflating: sport/sport_28.txt      \n",
            "  inflating: sport/sport_29.txt      \n",
            "  inflating: sport/sport_3.txt       \n",
            "  inflating: sport/sport_30.txt      \n",
            "  inflating: sport/sport_31.txt      \n",
            "  inflating: sport/sport_32.txt      \n",
            "  inflating: sport/sport_33.txt      \n",
            "  inflating: sport/sport_34.txt      \n",
            "  inflating: sport/sport_35.txt      \n",
            "  inflating: sport/sport_36.txt      \n",
            "  inflating: sport/sport_37.txt      \n",
            "  inflating: sport/sport_38.txt      \n",
            "  inflating: sport/sport_39.txt      \n",
            "  inflating: sport/sport_4.txt       \n",
            "  inflating: sport/sport_40.txt      \n",
            "  inflating: sport/sport_41.txt      \n",
            "  inflating: sport/sport_42.txt      \n",
            "  inflating: sport/sport_43.txt      \n",
            "  inflating: sport/sport_44.txt      \n",
            "  inflating: sport/sport_45.txt      \n",
            "  inflating: sport/sport_46.txt      \n",
            "  inflating: sport/sport_47.txt      \n",
            "  inflating: sport/sport_48.txt      \n",
            "  inflating: sport/sport_49.txt      \n",
            "  inflating: sport/sport_5.txt       \n",
            "  inflating: sport/sport_50.txt      \n",
            "  inflating: sport/sport_51.txt      \n",
            "  inflating: sport/sport_52.txt      \n",
            "  inflating: sport/sport_53.txt      \n",
            "  inflating: sport/sport_54.txt      \n",
            "  inflating: sport/sport_55.txt      \n",
            "  inflating: sport/sport_56.txt      \n",
            "  inflating: sport/sport_57.txt      \n",
            "  inflating: sport/sport_58.txt      \n",
            "  inflating: sport/sport_59.txt      \n",
            "  inflating: sport/sport_6.txt       \n",
            "  inflating: sport/sport_60.txt      \n",
            "  inflating: sport/sport_61.txt      \n",
            "  inflating: sport/sport_62.txt      \n",
            "  inflating: sport/sport_63.txt      \n",
            "  inflating: sport/sport_64.txt      \n",
            "  inflating: sport/sport_65.txt      \n",
            "  inflating: sport/sport_66.txt      \n",
            "  inflating: sport/sport_67.txt      \n",
            "  inflating: sport/sport_68.txt      \n",
            "  inflating: sport/sport_69.txt      \n",
            "  inflating: sport/sport_7.txt       \n",
            "  inflating: sport/sport_70.txt      \n",
            "  inflating: sport/sport_71.txt      \n",
            "  inflating: sport/sport_72.txt      \n",
            "  inflating: sport/sport_73.txt      \n",
            "  inflating: sport/sport_74.txt      \n",
            "  inflating: sport/sport_75.txt      \n",
            "  inflating: sport/sport_76.txt      \n",
            "  inflating: sport/sport_77.txt      \n",
            "  inflating: sport/sport_78.txt      \n",
            "  inflating: sport/sport_79.txt      \n",
            "  inflating: sport/sport_8.txt       \n",
            "  inflating: sport/sport_80.txt      \n",
            "  inflating: sport/sport_81.txt      \n",
            "  inflating: sport/sport_82.txt      \n",
            "  inflating: sport/sport_83.txt      \n",
            "  inflating: sport/sport_84.txt      \n",
            "  inflating: sport/sport_85.txt      \n",
            "  inflating: sport/sport_86.txt      \n",
            "  inflating: sport/sport_87.txt      \n",
            "  inflating: sport/sport_88.txt      \n",
            "  inflating: sport/sport_89.txt      \n",
            "  inflating: sport/sport_9.txt       \n",
            "  inflating: sport/sport_90.txt      \n",
            "  inflating: sport/sport_91.txt      \n",
            "  inflating: sport/sport_92.txt      \n",
            "  inflating: sport/sport_93.txt      \n",
            "  inflating: sport/sport_94.txt      \n",
            "  inflating: sport/sport_95.txt      \n",
            "  inflating: sport/sport_96.txt      \n",
            "  inflating: sport/sport_97.txt      \n",
            "  inflating: sport/sport_98.txt      \n",
            "  inflating: sport/sport_99.txt      \n",
            "  inflating: technologie/technologie_1.txt  \n",
            "  inflating: technologie/technologie_10.txt  \n",
            "  inflating: technologie/technologie_100.txt  \n",
            "  inflating: technologie/technologie_11.txt  \n",
            "  inflating: technologie/technologie_12.txt  \n",
            "  inflating: technologie/technologie_13.txt  \n",
            "  inflating: technologie/technologie_14.txt  \n",
            "  inflating: technologie/technologie_15.txt  \n",
            "  inflating: technologie/technologie_16.txt  \n",
            "  inflating: technologie/technologie_17.txt  \n",
            "  inflating: technologie/technologie_18.txt  \n",
            "  inflating: technologie/technologie_19.txt  \n",
            "  inflating: technologie/technologie_2.txt  \n",
            "  inflating: technologie/technologie_20.txt  \n",
            "  inflating: technologie/technologie_21.txt  \n",
            "  inflating: technologie/technologie_22.txt  \n",
            "  inflating: technologie/technologie_23.txt  \n",
            "  inflating: technologie/technologie_24.txt  \n",
            "  inflating: technologie/technologie_25.txt  \n",
            "  inflating: technologie/technologie_26.txt  \n",
            "  inflating: technologie/technologie_27.txt  \n",
            "  inflating: technologie/technologie_28.txt  \n",
            "  inflating: technologie/technologie_29.txt  \n",
            "  inflating: technologie/technologie_3.txt  \n",
            "  inflating: technologie/technologie_30.txt  \n",
            "  inflating: technologie/technologie_31.txt  \n",
            "  inflating: technologie/technologie_32.txt  \n",
            "  inflating: technologie/technologie_33.txt  \n",
            "  inflating: technologie/technologie_34.txt  \n",
            "  inflating: technologie/technologie_35.txt  \n",
            "  inflating: technologie/technologie_36.txt  \n",
            "  inflating: technologie/technologie_37.txt  \n",
            "  inflating: technologie/technologie_38.txt  \n",
            "  inflating: technologie/technologie_39.txt  \n",
            "  inflating: technologie/technologie_4.txt  \n",
            "  inflating: technologie/technologie_40.txt  \n",
            "  inflating: technologie/technologie_41.txt  \n",
            "  inflating: technologie/technologie_42.txt  \n",
            "  inflating: technologie/technologie_43.txt  \n",
            "  inflating: technologie/technologie_44.txt  \n",
            "  inflating: technologie/technologie_45.txt  \n",
            "  inflating: technologie/technologie_46.txt  \n",
            "  inflating: technologie/technologie_47.txt  \n",
            "  inflating: technologie/technologie_48.txt  \n",
            "  inflating: technologie/technologie_49.txt  \n",
            "  inflating: technologie/technologie_5.txt  \n",
            "  inflating: technologie/technologie_50.txt  \n",
            "  inflating: technologie/technologie_51.txt  \n",
            "  inflating: technologie/technologie_52.txt  \n",
            "  inflating: technologie/technologie_53.txt  \n",
            "  inflating: technologie/technologie_54.txt  \n",
            "  inflating: technologie/technologie_55.txt  \n",
            "  inflating: technologie/technologie_56.txt  \n",
            "  inflating: technologie/technologie_57.txt  \n",
            "  inflating: technologie/technologie_58.txt  \n",
            "  inflating: technologie/technologie_59.txt  \n",
            "  inflating: technologie/technologie_6.txt  \n",
            "  inflating: technologie/technologie_60.txt  \n",
            "  inflating: technologie/technologie_61.txt  \n",
            "  inflating: technologie/technologie_62.txt  \n",
            "  inflating: technologie/technologie_63.txt  \n",
            "  inflating: technologie/technologie_64.txt  \n",
            "  inflating: technologie/technologie_65.txt  \n",
            "  inflating: technologie/technologie_66.txt  \n",
            "  inflating: technologie/technologie_67.txt  \n",
            "  inflating: technologie/technologie_68.txt  \n",
            "  inflating: technologie/technologie_69.txt  \n",
            "  inflating: technologie/technologie_7.txt  \n",
            "  inflating: technologie/technologie_70.txt  \n",
            "  inflating: technologie/technologie_71.txt  \n",
            "  inflating: technologie/technologie_72.txt  \n",
            "  inflating: technologie/technologie_73.txt  \n",
            "  inflating: technologie/technologie_74.txt  \n",
            "  inflating: technologie/technologie_75.txt  \n",
            "  inflating: technologie/technologie_76.txt  \n",
            "  inflating: technologie/technologie_77.txt  \n",
            "  inflating: technologie/technologie_78.txt  \n",
            "  inflating: technologie/technologie_79.txt  \n",
            "  inflating: technologie/technologie_8.txt  \n",
            "  inflating: technologie/technologie_80.txt  \n",
            "  inflating: technologie/technologie_81.txt  \n",
            "  inflating: technologie/technologie_82.txt  \n",
            "  inflating: technologie/technologie_83.txt  \n",
            "  inflating: technologie/technologie_84.txt  \n",
            "  inflating: technologie/technologie_85.txt  \n",
            "  inflating: technologie/technologie_86.txt  \n",
            "  inflating: technologie/technologie_87.txt  \n",
            "  inflating: technologie/technologie_88.txt  \n",
            "  inflating: technologie/technologie_89.txt  \n",
            "  inflating: technologie/technologie_9.txt  \n",
            "  inflating: technologie/technologie_90.txt  \n",
            "  inflating: technologie/technologie_91.txt  \n",
            "  inflating: technologie/technologie_92.txt  \n",
            "  inflating: technologie/technologie_93.txt  \n",
            "  inflating: technologie/technologie_94.txt  \n",
            "  inflating: technologie/technologie_95.txt  \n",
            "  inflating: technologie/technologie_96.txt  \n",
            "  inflating: technologie/technologie_97.txt  \n",
            "  inflating: technologie/technologie_98.txt  \n",
            "  inflating: technologie/technologie_99.txt  \n",
            "business     entertainment  graphics\tmedical   sample_data  sport\n",
            "DATASET.zip  food\t    historical\tpolitics  space        technologie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loading and Preprocessing\n"
      ],
      "metadata": {
        "id": "7PISyrcFElWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extrcting each category, setting 30% of data for training and 70% for testing"
      ],
      "metadata": {
        "id": "o2D4UpEyJtuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of categories based on the directories\n",
        "categories = ['business', 'entertainment', 'food', 'graphics', 'historical', 'medical', 'politics', 'space', 'sport', 'technologie']\n",
        "\n",
        "texts = []\n",
        "labels = []\n",
        "\n",
        "for category in categories:\n",
        "    for filename in os.listdir(category):\n",
        "        with open(f\"./{category}/{filename}\", 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            texts.append(file.read())\n",
        "            labels.append(category)\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "6dlm8WsREn75"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Naive Bayes, SVM, Random Forest, Deep Neural Network (No enhancment tecniques)**"
      ],
      "metadata": {
        "id": "kQKcJMZQSTZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description:**\n",
        "\n",
        "This code segment is dedicated to enhancing the document classification process by incorporating bigrams (pairs of adjacent words) and evaluating the performance of various machine learning models, including a deep neural network.\n",
        "\n",
        "**Bigram Incorporation:**\n",
        "The TfidfVectorizer is employed with the ngram_range parameter set to (1,2). This means that the vectorization process will consider both individual words (unigrams) and pairs of adjacent words (bigrams) to transform the text data into numerical format. This can capture more contextual information than using unigrams alone.\n",
        "\n",
        "The training and test datasets are transformed using this vectorizer and converted to arrays for compatibility with Keras.\n",
        "\n",
        "**Deep Neural Network Definition:**\n",
        "\n",
        "A feed-forward neural network is defined using the Keras library. The network comprises two hidden layers with dropout regularization to prevent overfitting. The output layer uses a softmax activation function, suitable for multi-class classification problems. The loss function chosen is sparse_categorical_crossentropy, which is appropriate for integer-encoded class labels.\n",
        "\n",
        "**Model Training and Evaluation:**\n",
        "\n",
        "Four models are defined: Naive Bayes, Support Vector Machine (SVM), Random Forest, and the previously defined Deep Neural Network.\n",
        "Each model is trained on the training dataset and then evaluated on the test dataset.\n",
        "The performance of each model is assessed using various metrics, including precision, recall, F1-score, accuracy, and support. These metrics provide a comprehensive understanding of each model's performance, considering both the positive and negative classes.\n",
        "The results are aggregated and presented in a tabular format using a DataFrame.\n",
        "\n",
        "**Outcome:**\n",
        "\n",
        "Upon execution, this code will display the performance metrics of each model when using bigrams as part of the feature extraction process. This provides insights into the potential benefits of capturing more contextual information from the text data.\n"
      ],
      "metadata": {
        "id": "k4DIoanDJUxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using only unigrams\n",
        "tfidf_vectorizer_ngrams = TfidfVectorizer(stop_words='english', ngram_range=(1,1))\n",
        "X_train_tfidf_ngrams = tfidf_vectorizer_ngrams.fit_transform(X_train).toarray()  # Convert to array for Keras\n",
        "X_test_tfidf_ngrams = tfidf_vectorizer_ngrams.transform(X_test).toarray()\n",
        "\n",
        "# Define a simple feed-forward neural network for Keras\n",
        "def create_nn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=X_train_tfidf_ngrams.shape[1], activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(len(set(y_train)), activation='softmax'))  # Number of classes\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def train_and_evaluate_models(X_train, X_test):\n",
        "    # Define the models\n",
        "    models = {\n",
        "        'Naive Bayes': MultinomialNB(),\n",
        "        'SVM': SVC(kernel='linear'),\n",
        "        'Random Forest': RandomForestClassifier(random_state=42),\n",
        "        'Deep Neural Network': KerasClassifier(build_fn=create_nn_model, epochs=10, batch_size=32, verbose=0)\n",
        "    }\n",
        "\n",
        "    # Train and evaluate the models\n",
        "    results = {}\n",
        "    for model_name, model in models.items():\n",
        "        # Train\n",
        "        model.fit(X_train, y_train)\n",
        "        # Predict\n",
        "        y_pred = model.predict(X_test)\n",
        "        # Evaluate\n",
        "        report = classification_report(y_test, y_pred, output_dict=True)\n",
        "        results[model_name] = {\n",
        "            'Precision': report['weighted avg']['precision'],\n",
        "            'Recall': report['weighted avg']['recall'],\n",
        "            'F1-Score': report['weighted avg']['f1-score'],\n",
        "            'Accuracy': report['accuracy'],\n",
        "            'Support': report['weighted avg']['support']\n",
        "        }\n",
        "\n",
        "    # Convert results to DataFrame for display\n",
        "    df_results = pd.DataFrame(results).transpose()\n",
        "    return df_results\n",
        "\n",
        "# Train and evaluate models with Unigrams\n",
        "df_results = train_and_evaluate_models(X_train_tfidf_ngrams, X_test_tfidf_ngrams)\n",
        "print(df_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQOpNzbAh0Fl",
        "outputId": "baa2e672-6344-4279-b98e-beee3ab5e406"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-016e070b4257>:23: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  'Deep Neural Network': KerasClassifier(build_fn=create_nn_model, epochs=10, batch_size=32, verbose=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 9ms/step\n",
            "                     Precision    Recall  F1-Score  Accuracy  Support\n",
            "Naive Bayes           0.961440  0.956667  0.957449  0.956667    300.0\n",
            "SVM                   0.981571  0.980000  0.980188  0.980000    300.0\n",
            "Random Forest         0.931606  0.923333  0.924426  0.923333    300.0\n",
            "Deep Neural Network   0.980673  0.980000  0.980028  0.980000    300.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Naive Bayes, SVM, Random Forest, Deep Neural Network (Feature Engineering - N-grams)**"
      ],
      "metadata": {
        "id": "4fVwnalCTGiL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description:**\n",
        "\n",
        "This segment of code is dedicated to the task of document classification, with a particular emphasis on leveraging bigrams (pairs of adjacent words) as features. The performance of various machine learning models, inclusive of a deep neural network, is evaluated.\n",
        "\n",
        "\n",
        "**Bigram Feature Extraction:**\n",
        "\n",
        "The TfidfVectorizer is utilized with the ngram_range parameter set to (1,2). This ensures that the vectorization process captures both individual words (unigrams) and their adjacent pairs (bigrams). This approach can potentially encapsulate more contextual nuances than merely using unigrams.\n",
        "The training and test datasets undergo transformation using this vectorizer and are subsequently converted to arrays to ensure compatibility with the Keras framework.\n",
        "\n",
        "\n",
        "**Deep Neural Network Architecture:**\n",
        "\n",
        "A feed-forward neural network is architected using the Keras library. The network encompasses two hidden layers, with dropout layers interspersed to mitigate the risk of overfitting. The activation function for the output layer is set to softmax, making it suitable for multi-class classification scenarios. The chosen loss function, sparse_categorical_crossentropy, is apt for integer-encoded class labels.\n",
        "\n",
        "**Model Training & Evaluation Framework:**\n",
        "\n",
        "Four distinct models are delineated: Naive Bayes, Support Vector Machine (SVM), Random Forest, and the previously defined Deep Neural Network.\n",
        "Each model undergoes training on the training dataset, followed by evaluation on the test dataset.\n",
        "\n",
        "Performance metrics, namely precision, recall, F1-score, accuracy, and support, are employed to gauge the efficacy of each model. These metrics furnish a holistic view of the model's performance, taking into account both the positive and negative classes.\n",
        "The results are collated and presented in a structured tabular format using a DataFrame.\n",
        "\n",
        "**Outcome:**\n",
        "\n",
        "Upon execution, this code will render the performance metrics of each model when bigrams are incorporated into the feature extraction process. This elucidates the potential advantages of imbibing more contextual information from the textual data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NyhDpMxPKSs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using bigrams\n",
        "tfidf_vectorizer_ngrams = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
        "X_train_tfidf_ngrams = tfidf_vectorizer_ngrams.fit_transform(X_train).toarray()  # Convert to array for Keras\n",
        "X_test_tfidf_ngrams = tfidf_vectorizer_ngrams.transform(X_test).toarray()\n",
        "\n",
        "# Define a simple feed-forward neural network for Keras\n",
        "def create_nn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=X_train_tfidf_ngrams.shape[1], activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(len(set(y_train)), activation='softmax'))  # Number of classes\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def train_and_evaluate_models(X_train, X_test):\n",
        "    # Define the models\n",
        "    models = {\n",
        "        'N-grams Naive Bayes': MultinomialNB(),\n",
        "        'N-grams SVM': SVC(kernel='linear'),\n",
        "        'N-grams Random Forest': RandomForestClassifier(random_state=42),\n",
        "        'N-grams Deep Neural Network': KerasClassifier(build_fn=create_nn_model, epochs=10, batch_size=32, verbose=0)\n",
        "    }\n",
        "\n",
        "    # Train and evaluate the models\n",
        "    results = {}\n",
        "    for model_name, model in models.items():\n",
        "        # Train\n",
        "        model.fit(X_train, y_train)\n",
        "        # Predict\n",
        "        y_pred = model.predict(X_test)\n",
        "        # Evaluate\n",
        "        report = classification_report(y_test, y_pred, output_dict=True)\n",
        "        results[model_name] = {\n",
        "            'Precision': report['weighted avg']['precision'],\n",
        "            'Recall': report['weighted avg']['recall'],\n",
        "            'F1-Score': report['weighted avg']['f1-score'],\n",
        "            'Accuracy': report['accuracy'],\n",
        "            'Support': report['weighted avg']['support']\n",
        "        }\n",
        "\n",
        "    # Convert results to DataFrame for display\n",
        "    df_results = pd.DataFrame(results).transpose()\n",
        "    return df_results\n",
        "\n",
        "# Train and evaluate models with N-grams\n",
        "df_results_ngrams = train_and_evaluate_models(X_train_tfidf_ngrams, X_test_tfidf_ngrams)\n",
        "print(df_results_ngrams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTST751LjO5D",
        "outputId": "644492d7-3b74-4934-9089-dd4a1a213a8a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-8a8ebceb1d20>:23: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  'N-grams Deep Neural Network': KerasClassifier(build_fn=create_nn_model, epochs=10, batch_size=32, verbose=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 1s 42ms/step\n",
            "                             Precision    Recall  F1-Score  Accuracy  Support\n",
            "N-grams Naive Bayes           0.955492  0.946667  0.948155  0.946667    300.0\n",
            "N-grams SVM                   0.980477  0.980000  0.979978  0.980000    300.0\n",
            "N-grams Random Forest         0.932119  0.926667  0.926885  0.926667    300.0\n",
            "N-grams Deep Neural Network   0.974340  0.970000  0.970701  0.970000    300.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Naive Bayes, SVM, Random Forest, Deep Neural Network (Word Embeddings - Pre-trained Embeddings)**"
      ],
      "metadata": {
        "id": "5k27dAWIUJn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description:**\n",
        "\n",
        "This segment of the code is dedicated to the task of document classification, leveraging the power of Word2Vec embeddings. Word2Vec is a pre-trained model that captures semantic relationships between words by representing them as vectors in a high-dimensional space. The performance of various machine learning models, inclusive of a deep neural network, is evaluated using these embeddings.\n",
        "\n",
        "**Word2Vec Embeddings:**\n",
        "\n",
        "The Google News Word2Vec model, which is trained on a vast corpus and has a vector size of 300, is loaded.\n",
        "A function, document_to_word2vec, is defined to convert a document into its corresponding Word2Vec representation. This is achieved by averaging the Word2Vec vectors of individual words present in the document.\n",
        "Both the training and test datasets are transformed into their Word2Vec representations.\n",
        "\n",
        "**Deep Neural Network Architecture:**\n",
        "\n",
        "A feed-forward neural network is architected using the Keras library. The network encompasses two hidden layers, with dropout layers interspersed to mitigate the risk of overfitting. The activation function for the output layer is set to softmax, making it suitable for multi-class classification scenarios. The chosen loss function, sparse_categorical_crossentropy, is apt for integer-encoded class labels.\n",
        "\n",
        "**Model Training & Evaluation Framework:**\n",
        "\n",
        "Four distinct models are delineated: Naive Bayes (specifically GaussianNB due to continuous features from Word2Vec), Support Vector Machine (SVM), Random Forest, and the previously defined Deep Neural Network.\n",
        "Each model undergoes training on the Word2Vec-transformed training dataset, followed by evaluation on the transformed test dataset.\n",
        "Performance metrics, namely precision, recall, F1-score, accuracy, and support, are employed to gauge the efficacy of each model. These metrics furnish a holistic view of the model's performance, taking into account both the positive and negative classes.\n",
        "The results are collated and presented in a structured tabular format using a DataFrame.\n",
        "\n",
        "**Outcome:**\n",
        "Upon execution, this code will render the performance metrics of each model when Word2Vec embeddings are employed as features. This elucidates the potential advantages of imbibing semantic information from the textual data.\n",
        "\n"
      ],
      "metadata": {
        "id": "v28ISuf5Pq4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def document_to_word2vec(doc, model):\n",
        "    # Tokenize the document, filter out words not in the model's vocabulary\n",
        "    words = [word for word in doc.split() if word in model.key_to_index]\n",
        "    if len(words) == 0:\n",
        "        return np.zeros(model.vector_size)\n",
        "    # Convert words to vectors and average them\n",
        "    return np.mean([model[word] for word in words], axis=0)\n",
        "\n",
        "X_train_word2vec = np.array([document_to_word2vec(doc, word2vec_model) for doc in X_train])\n",
        "X_test_word2vec = np.array([document_to_word2vec(doc, word2vec_model) for doc in X_test])\n",
        "\n",
        "# Define a simple feed-forward neural network for Keras\n",
        "def create_nn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=X_train_word2vec.shape[1], activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(len(set(y_train)), activation='softmax'))  # Number of classes\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def train_and_evaluate_models(X_train, X_test):\n",
        "    # Define the models\n",
        "    models = {\n",
        "        'Pre-trained Embeddings Naive Bayes': GaussianNB(),\n",
        "        'Pre-trained Embeddings SVM': SVC(kernel='linear'),\n",
        "        'Pre-trained Embeddings Random Forest': RandomForestClassifier(random_state=42),\n",
        "        'Pre-trained Embeddings Deep Neural Network': KerasClassifier(build_fn=create_nn_model, epochs=10, batch_size=32, verbose=0)\n",
        "    }\n",
        "\n",
        "    # Train and evaluate the models\n",
        "    results = {}\n",
        "    for model_name, model in models.items():\n",
        "        # Train\n",
        "        model.fit(X_train, y_train)\n",
        "        # Predict\n",
        "        y_pred = model.predict(X_test)\n",
        "        # Evaluate\n",
        "        report = classification_report(y_test, y_pred, output_dict=True)\n",
        "        results[model_name] = {\n",
        "            'Precision': report['weighted avg']['precision'],\n",
        "            'Recall': report['weighted avg']['recall'],\n",
        "            'F1-Score': report['weighted avg']['f1-score'],\n",
        "            'Accuracy': report['accuracy'],\n",
        "            'Support': report['weighted avg']['support']\n",
        "        }\n",
        "\n",
        "    # Convert results to DataFrame for display\n",
        "    df_results = pd.DataFrame(results).transpose()\n",
        "    return df_results\n",
        "\n",
        "# Train and evaluate models with Word2Vec features\n",
        "df_results_word2vec = train_and_evaluate_models(X_train_word2vec, X_test_word2vec)\n",
        "print(df_results_word2vec)\n"
      ],
      "metadata": {
        "id": "lvu3GO8pamNT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b508e32-d12a-425c-8bc5-923708209e34"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-3eff0dd18190>:29: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  'Pre-trained Embeddings Deep Neural Network': KerasClassifier(build_fn=create_nn_model, epochs=10, batch_size=32, verbose=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 3ms/step\n",
            "                                            Precision    Recall  F1-Score  \\\n",
            "Pre-trained Embeddings Naive Bayes           0.929992  0.920000  0.920931   \n",
            "Pre-trained Embeddings SVM                   0.956522  0.953333  0.953573   \n",
            "Pre-trained Embeddings Random Forest         0.945605  0.943333  0.943607   \n",
            "Pre-trained Embeddings Deep Neural Network   0.908773  0.903333  0.903642   \n",
            "\n",
            "                                            Accuracy  Support  \n",
            "Pre-trained Embeddings Naive Bayes          0.920000    300.0  \n",
            "Pre-trained Embeddings SVM                  0.953333    300.0  \n",
            "Pre-trained Embeddings Random Forest        0.943333    300.0  \n",
            "Pre-trained Embeddings Deep Neural Network  0.903333    300.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Naive Bayes, SVM, Random Forest, Deep Neural Network (Feature Selection - Chi-Squared Test)**"
      ],
      "metadata": {
        "id": "6-R0ipGuXOx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description:**\n",
        "\n",
        "In this segment, the code is focused on enhancing the document classification task by incorporating bigrams and feature selection using the chi-squared test. The objective is to discern the impact of these techniques on the performance of various machine learning models, including a deep neural network.\n",
        "\n",
        "**Bigrams with TF-IDF:**\n",
        "\n",
        "The Term Frequency-Inverse Document Frequency (TF-IDF) vectorizer is employed with a configuration to consider both unigrams and bigrams. Bigrams can capture more contextual information than unigrams, potentially improving the model's understanding of the text.\n",
        "The training and test datasets are transformed into their corresponding TF-IDF representations, which are then converted to arrays to facilitate compatibility with Keras.\n",
        "\n",
        "**Feature Selection using Chi-Squared Test:**\n",
        "\n",
        "The chi-squared test is a statistical test used to determine the dependence of two categorical variables. Here, it's used to select the top 10,000 features that have the strongest relationship with the output variable.\n",
        "The TF-IDF transformed datasets are further refined by retaining only the selected features.\n",
        "\n",
        "**Deep Neural Network Architecture:**\n",
        "\n",
        "A feed-forward neural network is constructed using the Keras library. The network comprises two hidden layers, with dropout layers interspersed to prevent overfitting. The softmax activation function in the output layer ensures compatibility with multi-class classification. The loss function, sparse_categorical_crossentropy, is suitable for integer-encoded class labels.\n",
        "\n",
        "**Model Training & Evaluation Framework:**\n",
        "\n",
        "Four distinct models are delineated: Naive Bayes, Support Vector Machine (SVM), Random Forest, and the previously defined Deep Neural Network.\n",
        "Each model undergoes training on the chi-squared selected features of the training dataset and is subsequently evaluated on the test dataset.\n",
        "Performance metrics, namely precision, recall, F1-score, accuracy, and support, are employed to gauge the efficacy of each model. These metrics provide a comprehensive assessment of the model's performance, considering both the positive and negative classes.\n",
        "The results are collated and presented in a structured tabular format using a DataFrame.\n",
        "\n",
        "**Outcome:**\n",
        "Upon execution, this code segment will render the performance metrics of each model when bigrams and chi-squared feature selection are employed. This will elucidate the potential advantages of these techniques in enhancing the model's understanding of the textual data.\n",
        "\n"
      ],
      "metadata": {
        "id": "EHQ03necQQ7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using bigrams\n",
        "tfidf_vectorizer_ngrams = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
        "X_train_tfidf_ngrams = tfidf_vectorizer_ngrams.fit_transform(X_train).toarray()  # Convert to array for Keras\n",
        "X_test_tfidf_ngrams = tfidf_vectorizer_ngrams.transform(X_test).toarray()\n",
        "\n",
        "# Select top 10,000 features based on the chi-squared test\n",
        "k_best = 10000\n",
        "ch2 = SelectKBest(chi2, k=k_best)\n",
        "X_train_chi2_selected = ch2.fit_transform(X_train_tfidf_ngrams, y_train)\n",
        "X_test_chi2_selected = ch2.transform(X_test_tfidf_ngrams)\n",
        "\n",
        "# Define a simple feed-forward neural network for Keras\n",
        "def create_nn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=X_train_chi2_selected.shape[1], activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(len(set(y_train)), activation='softmax'))  # Number of classes\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def train_and_evaluate_models(X_train, X_test):\n",
        "    # Define the models\n",
        "    models = {\n",
        "        'Chi-Squared Test Naive Bayes': MultinomialNB(),\n",
        "        'Chi-Squared Test SVM': SVC(kernel='linear'),\n",
        "        'Chi-Squared Test Random Forest': RandomForestClassifier(random_state=42),\n",
        "        'Chi-Squared Test Deep Neural Network': KerasClassifier(build_fn=create_nn_model, epochs=10, batch_size=32, verbose=0)\n",
        "    }\n",
        "\n",
        "    # Train and evaluate the models\n",
        "    results = {}\n",
        "    for model_name, model in models.items():\n",
        "        # Train\n",
        "        model.fit(X_train, y_train)\n",
        "        # Predict\n",
        "        y_pred = model.predict(X_test)\n",
        "        # Evaluate\n",
        "        report = classification_report(y_test, y_pred, output_dict=True)\n",
        "        results[model_name] = {\n",
        "            'Precision': report['weighted avg']['precision'],\n",
        "            'Recall': report['weighted avg']['recall'],\n",
        "            'F1-Score': report['weighted avg']['f1-score'],\n",
        "            'Accuracy': report['accuracy'],\n",
        "            'Support': report['weighted avg']['support']\n",
        "        }\n",
        "\n",
        "    # Convert results to DataFrame for display\n",
        "    df_results = pd.DataFrame(results).transpose()\n",
        "    return df_results\n",
        "\n",
        "# Train and evaluate models with chi-squared selected features\n",
        "df_results_chi2 = train_and_evaluate_models(X_train_chi2_selected, X_test_chi2_selected)\n",
        "print(df_results_chi2)\n"
      ],
      "metadata": {
        "id": "rXT69phPm0dx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1200f6cc-e498-4714-e7f4-8231f1bf9677"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-3714ebf6d361>:29: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  'Chi-Squared Test Deep Neural Network': KerasClassifier(build_fn=create_nn_model, epochs=10, batch_size=32, verbose=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 5ms/step\n",
            "                                      Precision    Recall  F1-Score  Accuracy  \\\n",
            "Chi-Squared Test Naive Bayes           0.964585  0.960000  0.960622  0.960000   \n",
            "Chi-Squared Test SVM                   0.974356  0.973333  0.973402  0.973333   \n",
            "Chi-Squared Test Random Forest         0.951898  0.946667  0.947257  0.946667   \n",
            "Chi-Squared Test Deep Neural Network   0.980575  0.980000  0.980029  0.980000   \n",
            "\n",
            "                                      Support  \n",
            "Chi-Squared Test Naive Bayes            300.0  \n",
            "Chi-Squared Test SVM                    300.0  \n",
            "Chi-Squared Test Random Forest          300.0  \n",
            "Chi-Squared Test Deep Neural Network    300.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Naive Bayes, SVM, Random Forest, Deep Neural Network (Model Ensembling - Bagging)**"
      ],
      "metadata": {
        "id": "lJI0GE-5cTSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description:**\n",
        "\n",
        "In this segment, the code is centered on exploring the benefits of ensemble learning, specifically bagging, to enhance the document classification task. Bagging, or Bootstrap Aggregating, involves training multiple instances of a model on different subsets of the training data and then aggregating their predictions. This can reduce variance and improve generalization.\n",
        "\n",
        "**Deep Neural Network Architecture:**\n",
        "\n",
        "A feed-forward neural network is constructed using the Keras library. The network comprises two hidden layers, with dropout layers interspersed to prevent overfitting. The softmax activation function in the output layer ensures compatibility with multi-class classification. The loss function, sparse_categorical_crossentropy, is suitable for integer-encoded class labels.\n",
        "\n",
        "**Bagging with Deep Neural Network:**\n",
        "\n",
        "The BaggingClassifier from scikit-learn is employed to create an ensemble of the previously defined deep neural network. The ensemble consists of 10 instances of the neural network, each trained on a different subset of the chi-squared selected features of the training dataset.\n",
        "The ensemble is trained and subsequently evaluated on the test dataset.\n",
        "\n",
        "**Consolidation of Results:**\n",
        "\n",
        "The performance metrics of the bagging ensemble for the deep neural network are consolidated with those of other models (SVM, Random Forest, Naive Bayes) that have presumably been evaluated using bagging in prior code segments.\n",
        "Metrics such as precision, recall, F1-score, accuracy, and support are considered for a comprehensive assessment of the model's performance.\n",
        "The consolidated results are structured in a tabular format using a DataFrame for a clear and concise presentation.\n",
        "\n",
        "**Outcome:**\n",
        "Upon execution, this code segment will render the performance metrics of each model when bagging is employed. This will provide insights into the potential advantages of ensemble learning in enhancing the model's robustness and generalization capabilities.\n",
        "\n"
      ],
      "metadata": {
        "id": "qjLgdYv5QvFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Bagging with SVM\n",
        "bagging_svm = BaggingClassifier(base_estimator=SVC(kernel='linear'), n_estimators=10, random_state=42)\n",
        "bagging_svm.fit(X_train_chi2_selected, y_train)\n",
        "y_pred_bagging_svm = bagging_svm.predict(X_test_chi2_selected)\n",
        "report_bagging_svm = classification_report(y_test, y_pred_bagging_svm, output_dict=True)\n",
        "\n",
        "# 2. Bagging with Random Forest\n",
        "bagging_rf = BaggingClassifier(base_estimator=RandomForestClassifier(random_state=42), n_estimators=10, random_state=42)\n",
        "bagging_rf.fit(X_train_chi2_selected, y_train)\n",
        "y_pred_bagging_rf = bagging_rf.predict(X_test_chi2_selected)\n",
        "report_bagging_rf = classification_report(y_test, y_pred_bagging_rf, output_dict=True)\n",
        "\n",
        "# 3. Bagging with Naive Bayes\n",
        "bagging_nb = BaggingClassifier(base_estimator=MultinomialNB(), n_estimators=10, random_state=42)\n",
        "bagging_nb.fit(X_train_chi2_selected, y_train)\n",
        "y_pred_bagging_nb = bagging_nb.predict(X_test_chi2_selected)\n",
        "report_bagging_nb = classification_report(y_test, y_pred_bagging_nb, output_dict=True)\n",
        "\n",
        "\n",
        "\n",
        "# Define a simple feed-forward neural network for Keras\n",
        "def create_nn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=X_train_chi2_selected.shape[1], activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(len(set(y_train)), activation='softmax'))  # Number of classes\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Bagging with Deep Neural Network\n",
        "bagging_dnn = BaggingClassifier(base_estimator=KerasClassifier(build_fn=create_nn_model, epochs=10, batch_size=32, verbose=0),\n",
        "                                n_estimators=10, random_state=42)\n",
        "bagging_dnn.fit(X_train_chi2_selected, y_train)\n",
        "y_pred_bagging_dnn = bagging_dnn.predict(X_test_chi2_selected)\n",
        "report_bagging_dnn = classification_report(y_test, y_pred_bagging_dnn, output_dict=True)\n",
        "\n",
        "# Consolidate and Display Results\n",
        "results_bagging = {\n",
        "    'Bagging SVM': {\n",
        "        'Precision': report_bagging_svm['weighted avg']['precision'],\n",
        "        'Recall': report_bagging_svm['weighted avg']['recall'],\n",
        "        'F1-Score': report_bagging_svm['weighted avg']['f1-score'],\n",
        "        'Accuracy': report_bagging_svm['accuracy'],\n",
        "        'Support': report_bagging_svm['weighted avg']['support']\n",
        "    },\n",
        "    'Bagging Random Forest': {\n",
        "        'Precision': report_bagging_rf['weighted avg']['precision'],\n",
        "        'Recall': report_bagging_rf['weighted avg']['recall'],\n",
        "        'F1-Score': report_bagging_rf['weighted avg']['f1-score'],\n",
        "        'Accuracy': report_bagging_rf['accuracy'],\n",
        "        'Support': report_bagging_rf['weighted avg']['support']\n",
        "    },\n",
        "    'Bagging Naive Bayes': {\n",
        "        'Precision': report_bagging_nb['weighted avg']['precision'],\n",
        "        'Recall': report_bagging_nb['weighted avg']['recall'],\n",
        "        'F1-Score': report_bagging_nb['weighted avg']['f1-score'],\n",
        "        'Accuracy': report_bagging_nb['accuracy'],\n",
        "        'Support': report_bagging_nb['weighted avg']['support']\n",
        "    },\n",
        "    'Bagging Deep Neural Network': {\n",
        "        'Precision': report_bagging_dnn['weighted avg']['precision'],\n",
        "        'Recall': report_bagging_dnn['weighted avg']['recall'],\n",
        "        'F1-Score': report_bagging_dnn['weighted avg']['f1-score'],\n",
        "        'Accuracy': report_bagging_dnn['accuracy'],\n",
        "        'Support': report_bagging_dnn['weighted avg']['support']\n",
        "    }\n",
        "}\n",
        "\n",
        "# Convert results to DataFrame for display\n",
        "df_results_bagging = pd.DataFrame(results_bagging).transpose()\n",
        "print(df_results_bagging)\n"
      ],
      "metadata": {
        "id": "QhLz0Z0XoYjZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2cb7304-d0e2-45a4-9cc3-905005ec6756"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "<ipython-input-12-c2671a90b19a>:33: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  bagging_dnn = BaggingClassifier(base_estimator=KerasClassifier(build_fn=create_nn_model, epochs=10, batch_size=32, verbose=0),\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 8ms/step\n",
            "10/10 [==============================] - 0s 5ms/step\n",
            "10/10 [==============================] - 0s 4ms/step\n",
            "10/10 [==============================] - 0s 4ms/step\n",
            "10/10 [==============================] - 0s 5ms/step\n",
            "10/10 [==============================] - 0s 5ms/step\n",
            "10/10 [==============================] - 0s 5ms/step\n",
            "10/10 [==============================] - 0s 4ms/step\n",
            "10/10 [==============================] - 0s 4ms/step\n",
            "10/10 [==============================] - 0s 4ms/step\n",
            "                             Precision    Recall  F1-Score  Accuracy  Support\n",
            "Bagging SVM                   0.968281  0.966667  0.966811  0.966667    300.0\n",
            "Bagging Random Forest         0.947732  0.940000  0.941160  0.940000    300.0\n",
            "Bagging Naive Bayes           0.955492  0.946667  0.948155  0.946667    300.0\n",
            "Bagging Deep Neural Network   0.971912  0.970000  0.970361  0.970000    300.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Naive Bayes, SVM, Random Forest, Deep Neural Network (Regularization Techniques - Dropout)**"
      ],
      "metadata": {
        "id": "56Z4ZTaZnmL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description:**\n",
        "\n",
        "In this segment, the primary focus is on the exploration of regularization techniques to improve the generalization capabilities of various machine learning models for document classification. Regularization helps in preventing overfitting, ensuring that the model performs well on unseen data.\n",
        "\n",
        "**Deep Neural Network with Dropout Regularization:**\n",
        "\n",
        "A feed-forward neural network is constructed using the TensorFlow and Keras libraries. Dropout layers are introduced after the dense layers to serve as a regularization mechanism. By randomly setting a fraction of input units to 0 at each update during training, dropout helps prevent overfitting.\n",
        "The network is trained on the chi-squared selected features of the training dataset and subsequently evaluated on the test dataset.\n",
        "\n",
        "**Support Vector Machines (SVM) with L1 and L2 Regularization:**\n",
        "\n",
        "Two SVM models are trained: one with L1 regularization and the other with L2 regularization. L1 regularization can lead to feature selection as it tends to produce a sparse weight vector, while L2 regularization can prevent overfitting without necessarily zeroing out weights.\n",
        "\n",
        "**Naive Bayes with Regularization:**\n",
        "\n",
        "A Naive Bayes classifier is trained with Laplace smoothing (controlled by the alpha parameter). This regularization technique helps in handling the absence of features in the training data that might appear in the test data.\n",
        "\n",
        "**Random Forest with Regularization:**\n",
        "\n",
        "A Random Forest classifier is trained with hyperparameters that act as regularization. The max_depth parameter ensures that the trees do not grow too deep, and the max_features parameter controls the number of features to consider when looking for the best split.\n",
        "\n",
        "The performance metrics of all models, including precision, recall, F1-score, accuracy, and support, are consolidated into a structured format.\n",
        "The results are presented in a tabular format using a DataFrame for clarity.\n",
        "\n",
        "**Outcome:**\n",
        "Upon execution, this code segment will display the performance metrics of each model with their respective regularization techniques. This will provide insights into the efficacy of regularization in enhancing the model's robustness and performance on unseen data.\n",
        "\n"
      ],
      "metadata": {
        "id": "7pwZfycWRSs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple feed-forward neural network for Keras with dropout as regularization\n",
        "def create_nn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=X_train_chi2_selected.shape[1], activation='relu'))\n",
        "    model.add(Dropout(0.5))  # Dropout layer for regularization\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))  # Dropout layer for regularization\n",
        "    model.add(Dense(len(set(y_train)), activation='softmax'))  # Number of classes\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train the neural network\n",
        "nn_model = KerasClassifier(build_fn=create_nn_model, epochs=10, batch_size=32, verbose=0)\n",
        "nn_model.fit(X_train_chi2_selected, y_train)  # No need for .toarray()\n",
        "y_pred_nn = nn_model.predict(X_test_chi2_selected)\n",
        "report_nn = classification_report(y_test, y_pred_nn, output_dict=True)\n",
        "\n",
        "# SVM with L1 regularization\n",
        "svm_l1 = LinearSVC(penalty='l1', dual=False, C=1.0)\n",
        "svm_l1.fit(X_train_chi2_selected, y_train)\n",
        "y_pred_svm_l1 = svm_l1.predict(X_test_chi2_selected)\n",
        "report_svm_l1 = classification_report(y_test, y_pred_svm_l1, output_dict=True)\n",
        "\n",
        "# SVM with L2 regularization (default in SVC)\n",
        "svm_l2 = SVC(kernel='linear', C=1.0)\n",
        "svm_l2.fit(X_train_chi2_selected, y_train)\n",
        "y_pred_svm_l2 = svm_l2.predict(X_test_chi2_selected)\n",
        "report_svm_l2 = classification_report(y_test, y_pred_svm_l2, output_dict=True)\n",
        "\n",
        "# Naive Bayes with regularization (alpha parameter)\n",
        "nb_regularized = MultinomialNB(alpha=0.5)  # alpha=1.0 is default (Laplace smoothing)\n",
        "nb_regularized.fit(X_train_chi2_selected, y_train)\n",
        "y_pred_nb_regularized = nb_regularized.predict(X_test_chi2_selected)\n",
        "report_nb_regularized = classification_report(y_test, y_pred_nb_regularized, output_dict=True)\n",
        "\n",
        "# Random Forest with hyperparameters acting as regularization\n",
        "rf_regularized = RandomForestClassifier(n_estimators=100, max_depth=10, max_features='sqrt', random_state=42)\n",
        "rf_regularized.fit(X_train_chi2_selected, y_train)\n",
        "y_pred_rf_regularized = rf_regularized.predict(X_test_chi2_selected)\n",
        "report_rf_regularized = classification_report(y_test, y_pred_rf_regularized, output_dict=True)\n",
        "\n",
        "# Consolidate and Display Results\n",
        "results_regularized = {\n",
        "    'Dropout SVM L1': {\n",
        "        'Precision': report_svm_l1['weighted avg']['precision'],\n",
        "        'Recall': report_svm_l1['weighted avg']['recall'],\n",
        "        'F1-Score': report_svm_l1['weighted avg']['f1-score'],\n",
        "        'Accuracy': report_svm_l1['accuracy'],\n",
        "        'Support': report_svm_l1['weighted avg']['support']\n",
        "    },\n",
        "    'Dropout SVM L2': {\n",
        "        'Precision': report_svm_l2['weighted avg']['precision'],\n",
        "        'Recall': report_svm_l2['weighted avg']['recall'],\n",
        "        'F1-Score': report_svm_l2['weighted avg']['f1-score'],\n",
        "        'Accuracy': report_svm_l2['accuracy'],\n",
        "        'Support': report_svm_l2['weighted avg']['support']\n",
        "    },\n",
        "    'Dropout Naive Bayes Regularized': {\n",
        "        'Precision': report_nb_regularized['weighted avg']['precision'],\n",
        "        'Recall': report_nb_regularized['weighted avg']['recall'],\n",
        "        'F1-Score': report_nb_regularized['weighted avg']['f1-score'],\n",
        "        'Accuracy': report_nb_regularized['accuracy'],\n",
        "        'Support': report_nb_regularized['weighted avg']['support']\n",
        "    },\n",
        "    'Dropout Random Forest Regularized': {\n",
        "        'Precision': report_rf_regularized['weighted avg']['precision'],\n",
        "        'Recall': report_rf_regularized['weighted avg']['recall'],\n",
        "        'F1-Score': report_rf_regularized['weighted avg']['f1-score'],\n",
        "        'Accuracy': report_rf_regularized['accuracy'],\n",
        "        'Support': report_rf_regularized['weighted avg']['support']\n",
        "    },\n",
        "    'Dropout Deep Neural Network': {\n",
        "        'Precision': report_nn['weighted avg']['precision'],\n",
        "        'Recall': report_nn['weighted avg']['recall'],\n",
        "        'F1-Score': report_nn['weighted avg']['f1-score'],\n",
        "        'Accuracy': report_nn['accuracy'],\n",
        "        'Support': report_nn['weighted avg']['support']\n",
        "    }\n",
        "}\n",
        "\n",
        "# Convert results to DataFrame for display\n",
        "df_results_regularized = pd.DataFrame(results_regularized).transpose()\n",
        "print(df_results_regularized)\n"
      ],
      "metadata": {
        "id": "SRey9kdVcZlE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d8f0ee6-5799-41d7-a979-1af589b7931c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-da01b0996c9c>:13: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  nn_model = KerasClassifier(build_fn=create_nn_model, epochs=10, batch_size=32, verbose=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 5ms/step\n",
            "                                   Precision    Recall  F1-Score  Accuracy  \\\n",
            "Dropout SVM L1                      0.918721  0.916667  0.916141  0.916667   \n",
            "Dropout SVM L2                      0.974356  0.973333  0.973402  0.973333   \n",
            "Dropout Naive Bayes Regularized     0.964585  0.960000  0.960622  0.960000   \n",
            "Dropout Random Forest Regularized   0.909965  0.903333  0.903895  0.903333   \n",
            "Dropout Deep Neural Network         0.984385  0.983333  0.983524  0.983333   \n",
            "\n",
            "                                   Support  \n",
            "Dropout SVM L1                       300.0  \n",
            "Dropout SVM L2                       300.0  \n",
            "Dropout Naive Bayes Regularized      300.0  \n",
            "Dropout Random Forest Regularized    300.0  \n",
            "Dropout Deep Neural Network          300.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Best Model without combinations**"
      ],
      "metadata": {
        "id": "QgUcVZ1lq_Oe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description:**\n",
        "\n",
        "In this code segment, the primary objective is to consolidate the results from various model configurations and preprocessing techniques, and then identify the best-performing model based on the F1-Score.\n",
        "\n",
        "**Consolidation of Results:**\n",
        "\n",
        "The results from different model configurations and preprocessing techniques, namely standard models, models with N-grams, models using Word2Vec embeddings, models with chi-squared selected features, models with bagging, and models with regularization, are consolidated into a single DataFrame.\n",
        "This consolidation provides a unified view, making it easier to compare and analyze the performance of different configurations side by side.\n",
        "\n",
        "**Sorting based on F1-Score:**\n",
        "\n",
        "The consolidated results are sorted in descending order based on the F1-Score. The F1-Score is a harmonic mean of precision and recall, providing a balanced measure of a model's performance, especially in cases where class distributions might be imbalanced.\n",
        "\n",
        "**Display of Results:**\n",
        "\n",
        "The sorted results are displayed in tabular format, providing a clear view of how each model configuration performed relative to the others.\n",
        "\n",
        "**Identification of the Best Model:**\n",
        "\n",
        "The model with the highest F1-Score is identified as the best model.\n",
        "Details of the best model, including its name and F1-Score, are displayed.\n",
        "\n",
        "**Outcome:**\n",
        "Upon execution, this code segment will present a comprehensive view of the performance metrics of all model configurations. It will also highlight the best-performing model based on the F1-Score, providing a clear recommendation for the most effective model configuration for the given task.\n",
        "\n"
      ],
      "metadata": {
        "id": "HyyfvHtSRxpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Consolidate all results\n",
        "all_results = pd.concat([\n",
        "    df_results,\n",
        "    df_results_ngrams,\n",
        "    df_results_word2vec,\n",
        "    df_results_chi2,\n",
        "    df_results_bagging,\n",
        "    df_results_regularized,\n",
        "])\n",
        "\n",
        "# Sort based on F1-Score\n",
        "sorted_results = all_results.sort_values(by='F1-Score', ascending=False)\n",
        "\n",
        "# Display the consolidated results\n",
        "print(sorted_results)\n",
        "\n",
        "# Display the best model\n",
        "best_model = sorted_results.iloc[0]\n",
        "print(\"\\nBest Model based on F1-Score:\")\n",
        "print(best_model.name)\n",
        "print(\"F1-Score:\", best_model['F1-Score'])\n"
      ],
      "metadata": {
        "id": "nKqiBFYuqff3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "214160d9-6a54-416b-bca7-92f747f261ba"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            Precision    Recall  F1-Score  \\\n",
            "Dropout Deep Neural Network                  0.984385  0.983333  0.983524   \n",
            "SVM                                          0.981571  0.980000  0.980188   \n",
            "Chi-Squared Test Deep Neural Network         0.980575  0.980000  0.980029   \n",
            "Deep Neural Network                          0.980673  0.980000  0.980028   \n",
            "N-grams SVM                                  0.980477  0.980000  0.979978   \n",
            "Dropout SVM L2                               0.974356  0.973333  0.973402   \n",
            "Chi-Squared Test SVM                         0.974356  0.973333  0.973402   \n",
            "N-grams Deep Neural Network                  0.974340  0.970000  0.970701   \n",
            "Bagging Deep Neural Network                  0.971912  0.970000  0.970361   \n",
            "Bagging SVM                                  0.968281  0.966667  0.966811   \n",
            "Dropout Naive Bayes Regularized              0.964585  0.960000  0.960622   \n",
            "Chi-Squared Test Naive Bayes                 0.964585  0.960000  0.960622   \n",
            "Naive Bayes                                  0.961440  0.956667  0.957449   \n",
            "Pre-trained Embeddings SVM                   0.956522  0.953333  0.953573   \n",
            "Bagging Naive Bayes                          0.955492  0.946667  0.948155   \n",
            "N-grams Naive Bayes                          0.955492  0.946667  0.948155   \n",
            "Chi-Squared Test Random Forest               0.951898  0.946667  0.947257   \n",
            "Pre-trained Embeddings Random Forest         0.945605  0.943333  0.943607   \n",
            "Bagging Random Forest                        0.947732  0.940000  0.941160   \n",
            "N-grams Random Forest                        0.932119  0.926667  0.926885   \n",
            "Random Forest                                0.931606  0.923333  0.924426   \n",
            "Pre-trained Embeddings Naive Bayes           0.929992  0.920000  0.920931   \n",
            "Dropout SVM L1                               0.918721  0.916667  0.916141   \n",
            "Dropout Random Forest Regularized            0.909965  0.903333  0.903895   \n",
            "Pre-trained Embeddings Deep Neural Network   0.908773  0.903333  0.903642   \n",
            "\n",
            "                                            Accuracy  Support  \n",
            "Dropout Deep Neural Network                 0.983333    300.0  \n",
            "SVM                                         0.980000    300.0  \n",
            "Chi-Squared Test Deep Neural Network        0.980000    300.0  \n",
            "Deep Neural Network                         0.980000    300.0  \n",
            "N-grams SVM                                 0.980000    300.0  \n",
            "Dropout SVM L2                              0.973333    300.0  \n",
            "Chi-Squared Test SVM                        0.973333    300.0  \n",
            "N-grams Deep Neural Network                 0.970000    300.0  \n",
            "Bagging Deep Neural Network                 0.970000    300.0  \n",
            "Bagging SVM                                 0.966667    300.0  \n",
            "Dropout Naive Bayes Regularized             0.960000    300.0  \n",
            "Chi-Squared Test Naive Bayes                0.960000    300.0  \n",
            "Naive Bayes                                 0.956667    300.0  \n",
            "Pre-trained Embeddings SVM                  0.953333    300.0  \n",
            "Bagging Naive Bayes                         0.946667    300.0  \n",
            "N-grams Naive Bayes                         0.946667    300.0  \n",
            "Chi-Squared Test Random Forest              0.946667    300.0  \n",
            "Pre-trained Embeddings Random Forest        0.943333    300.0  \n",
            "Bagging Random Forest                       0.940000    300.0  \n",
            "N-grams Random Forest                       0.926667    300.0  \n",
            "Random Forest                               0.923333    300.0  \n",
            "Pre-trained Embeddings Naive Bayes          0.920000    300.0  \n",
            "Dropout SVM L1                              0.916667    300.0  \n",
            "Dropout Random Forest Regularized           0.903333    300.0  \n",
            "Pre-trained Embeddings Deep Neural Network  0.903333    300.0  \n",
            "\n",
            "Best Model based on F1-Score:\n",
            "Dropout Deep Neural Network\n",
            "F1-Score: 0.9835244299799956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Best Model with combinations**"
      ],
      "metadata": {
        "id": "nvGcWdKEST4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description:**\n",
        "\n",
        "This code segment is designed to evaluate the performance of various machine learning models, combined with different preprocessing techniques, feature selection methods, and regularization techniques, on a given dataset. The primary objective is to identify the best-performing model configuration based on the F1-Score.\n",
        "\n",
        "**Neural Network Definition:**\n",
        "\n",
        "A simple feed-forward neural network is defined using Keras. This network will be used as one of the classifiers in the subsequent steps.\n",
        "\n",
        "**Model, Preprocessing, and Feature Selection Definitions:**\n",
        "\n",
        "Four machine learning models are defined: Linear SVM, Naive Bayes, Random Forest, and a Neural Network.\n",
        "Two preprocessing techniques are defined: TF-IDF and TF-IDF with N-grams.\n",
        "Two feature selection methods are defined: Chi-squared test and no selection.\n",
        "Two regularization techniques are defined for the Linear SVM: L1 and L2.\n",
        "\n",
        "**Training and Evaluation Loop:**\n",
        "\n",
        "A nested loop is used to iterate through each combination of model, preprocessing technique, feature selection method, and regularization technique.\n",
        "For each combination, the data is preprocessed, features are selected (if applicable), the model is trained, and predictions are made on the test set.\n",
        "Evaluation metrics (Precision, Recall, F1-Score, Accuracy, and Support) are calculated for each combination and stored in a list.\n",
        "\n",
        "**Results Consolidation and Display:**\n",
        "\n",
        "The results are consolidated into a DataFrame for easier visualization and analysis.\n",
        "The results are sorted based on the F1-Score in descending order to identify the best-performing model configuration.\n",
        "The consolidated results are displayed, followed by details of the best-performing model.\n",
        "\n",
        "**Outcome:**\n",
        "\n",
        "Upon execution, this code segment will provide a comprehensive view of the performance metrics of all model configurations. It will also highlight the best-performing model based on the F1-Score, offering insights into the most effective model configuration for the given task.\n",
        "\n"
      ],
      "metadata": {
        "id": "rDRULJOLSYRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to create the neural network model\n",
        "def create_nn_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Linear SVM': LinearSVC(),\n",
        "    'Naive Bayes': MultinomialNB(),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'Neural Network': None  # Placeholder, will be defined later\n",
        "}\n",
        "\n",
        "# Define preprocessing techniques\n",
        "preprocessing_techniques = {\n",
        "    'TF-IDF': TfidfVectorizer(stop_words='english'),\n",
        "    'TF-IDF Ngrams': TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
        "}\n",
        "\n",
        "# Define feature selection/enhancement techniques\n",
        "feature_selection_methods = {\n",
        "    'Chi-squared': SelectKBest(chi2, k=10000),\n",
        "    'No Selection': None\n",
        "}\n",
        "\n",
        "# Define regularization techniques (only for SVM)\n",
        "regularizations = {\n",
        "    'L1': {'penalty': 'l1', 'dual': False},\n",
        "    'L2': {'penalty': 'l2'}\n",
        "}\n",
        "\n",
        "results_list = []\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    for preprocess_name, preprocess in preprocessing_techniques.items():\n",
        "        for feature_name, feature_method in feature_selection_methods.items():\n",
        "\n",
        "            # Preprocess the data\n",
        "            X_train_preprocessed = preprocess.fit_transform(X_train)\n",
        "            X_test_preprocessed = preprocess.transform(X_test)\n",
        "\n",
        "            # Feature selection\n",
        "            if feature_method:\n",
        "                X_train_selected = feature_method.fit_transform(X_train_preprocessed, y_train)\n",
        "                X_test_selected = feature_method.transform(X_test_preprocessed)\n",
        "            else:\n",
        "                X_train_selected = X_train_preprocessed\n",
        "                X_test_selected = X_test_preprocessed\n",
        "\n",
        "            # Regularization (only for Linear SVM)\n",
        "            if model_name == 'Linear SVM':\n",
        "                for reg_name, reg_params in regularizations.items():\n",
        "                    model.set_params(**reg_params)\n",
        "\n",
        "                    # Train the model\n",
        "                    model.fit(X_train_selected, y_train)\n",
        "\n",
        "                    # Evaluate the model\n",
        "                    y_pred = model.predict(X_test_selected)\n",
        "\n",
        "                    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "                    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "                    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "                    accuracy = accuracy_score(y_test, y_pred)\n",
        "                    support = len(y_test)\n",
        "\n",
        "                    results_list.append({\n",
        "                        'Model': model_name,\n",
        "                        'Preprocessing': preprocess_name,\n",
        "                        'Feature Selection': feature_name,\n",
        "                        'Regularization': reg_name,\n",
        "                        'Precision': precision,\n",
        "                        'Recall': recall,\n",
        "                        'F1-Score': f1,\n",
        "                        'Accuracy': accuracy,\n",
        "                        'Support': support\n",
        "                    })\n",
        "\n",
        "            elif model_name == 'Neural Network':\n",
        "                input_dim = X_train_selected.shape[1]\n",
        "                model = KerasClassifier(build_fn=create_nn_model, input_dim=input_dim, epochs=10, batch_size=32, verbose=0)\n",
        "                model.fit(X_train_selected.toarray(), y_train)  # Convert sparse matrix to dense\n",
        "                y_pred = model.predict(X_test_selected.toarray())\n",
        "\n",
        "                precision = precision_score(y_test, y_pred, average='weighted')\n",
        "                recall = recall_score(y_test, y_pred, average='weighted')\n",
        "                f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                support = len(y_test)\n",
        "\n",
        "                results_list.append({\n",
        "                    'Model': model_name,\n",
        "                    'Preprocessing': preprocess_name,\n",
        "                    'Feature Selection': feature_name,\n",
        "                    'Regularization': 'N/A',\n",
        "                    'Precision': precision,\n",
        "                    'Recall': recall,\n",
        "                    'F1-Score': f1,\n",
        "                    'Accuracy': accuracy,\n",
        "                    'Support': support\n",
        "                })\n",
        "\n",
        "            else:\n",
        "                # Train the model\n",
        "                model.fit(X_train_selected, y_train)\n",
        "\n",
        "                # Evaluate the model\n",
        "                y_pred = model.predict(X_test_selected)\n",
        "\n",
        "                precision = precision_score(y_test, y_pred, average='weighted')\n",
        "                recall = recall_score(y_test, y_pred, average='weighted')\n",
        "                f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                support = len(y_test)\n",
        "\n",
        "                results_list.append({\n",
        "                    'Model': model_name,\n",
        "                    'Preprocessing': preprocess_name,\n",
        "                    'Feature Selection': feature_name,\n",
        "                    'Regularization': 'N/A',\n",
        "                    'Precision': precision,\n",
        "                    'Recall': recall,\n",
        "                    'F1-Score': f1,\n",
        "                    'Accuracy': accuracy,\n",
        "                    'Support': support\n",
        "                })\n",
        "\n",
        "# Convert results to DataFrame for display\n",
        "df_results = pd.DataFrame(results_list)\n",
        "\n",
        "# Sort based on F1-Score\n",
        "sorted_results = df_results.sort_values(by='F1-Score', ascending=False)\n",
        "\n",
        "# Display the consolidated results\n",
        "print(sorted_results)\n",
        "\n",
        "# Display the best model\n",
        "best_model = sorted_results.iloc[0]\n",
        "print(\"\\nBest Model based on F1-Score:\")\n",
        "print(best_model['Model'])\n",
        "print(\"F1-Score:\", best_model['F1-Score'])\n"
      ],
      "metadata": {
        "id": "KQdwV9NyDClJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb428617-73c0-4705-8812-ef216f89a202"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-a21d0e571704>:85: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn=create_nn_model, input_dim=input_dim, epochs=10, batch_size=32, verbose=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "<ipython-input-18-a21d0e571704>:85: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn=create_nn_model, input_dim=input_dim, epochs=10, batch_size=32, verbose=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 15ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "<ipython-input-18-a21d0e571704>:85: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn=create_nn_model, input_dim=input_dim, epochs=10, batch_size=32, verbose=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "<ipython-input-18-a21d0e571704>:85: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn=create_nn_model, input_dim=input_dim, epochs=10, batch_size=32, verbose=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 42ms/step\n",
            "             Model  Preprocessing Feature Selection Regularization  Precision  \\\n",
            "3       Linear SVM         TF-IDF      No Selection             L2   0.980477   \n",
            "5       Linear SVM  TF-IDF Ngrams       Chi-squared             L2   0.980477   \n",
            "7       Linear SVM  TF-IDF Ngrams      No Selection             L2   0.980477   \n",
            "1       Linear SVM         TF-IDF       Chi-squared             L2   0.980477   \n",
            "10     Naive Bayes  TF-IDF Ngrams       Chi-squared            N/A   0.964585   \n",
            "9      Naive Bayes         TF-IDF      No Selection            N/A   0.961440   \n",
            "8      Naive Bayes         TF-IDF       Chi-squared            N/A   0.961036   \n",
            "12   Random Forest         TF-IDF       Chi-squared            N/A   0.952299   \n",
            "11     Naive Bayes  TF-IDF Ngrams      No Selection            N/A   0.955492   \n",
            "14   Random Forest  TF-IDF Ngrams       Chi-squared            N/A   0.951898   \n",
            "15   Random Forest  TF-IDF Ngrams      No Selection            N/A   0.932119   \n",
            "13   Random Forest         TF-IDF      No Selection            N/A   0.931606   \n",
            "2       Linear SVM         TF-IDF      No Selection             L1   0.927104   \n",
            "0       Linear SVM         TF-IDF       Chi-squared             L1   0.923299   \n",
            "6       Linear SVM  TF-IDF Ngrams      No Selection             L1   0.918721   \n",
            "4       Linear SVM  TF-IDF Ngrams       Chi-squared             L1   0.918721   \n",
            "16  Neural Network         TF-IDF       Chi-squared            N/A   0.004444   \n",
            "17  Neural Network         TF-IDF      No Selection            N/A   0.004444   \n",
            "18  Neural Network  TF-IDF Ngrams       Chi-squared            N/A   0.004444   \n",
            "19  Neural Network  TF-IDF Ngrams      No Selection            N/A   0.004444   \n",
            "\n",
            "      Recall  F1-Score  Accuracy  Support  \n",
            "3   0.980000  0.979978  0.980000      300  \n",
            "5   0.980000  0.979978  0.980000      300  \n",
            "7   0.980000  0.979978  0.980000      300  \n",
            "1   0.980000  0.979978  0.980000      300  \n",
            "10  0.960000  0.960622  0.960000      300  \n",
            "9   0.956667  0.957449  0.956667      300  \n",
            "8   0.956667  0.957295  0.956667      300  \n",
            "12  0.950000  0.950181  0.950000      300  \n",
            "11  0.946667  0.948155  0.946667      300  \n",
            "14  0.946667  0.947257  0.946667      300  \n",
            "15  0.926667  0.926885  0.926667      300  \n",
            "13  0.923333  0.924426  0.923333      300  \n",
            "2   0.923333  0.923015  0.923333      300  \n",
            "0   0.920000  0.919375  0.920000      300  \n",
            "6   0.916667  0.916141  0.916667      300  \n",
            "4   0.916667  0.916141  0.916667      300  \n",
            "16  0.066667  0.008333  0.066667      300  \n",
            "17  0.066667  0.008333  0.066667      300  \n",
            "18  0.066667  0.008333  0.066667      300  \n",
            "19  0.066667  0.008333  0.066667      300  \n",
            "\n",
            "Best Model based on F1-Score:\n",
            "Linear SVM\n",
            "F1-Score: 0.9799776743044223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}